
# Version: v6.0 (Enhanced with LLM Factory) - Ultra Jennie LLM ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ëª¨ë“ˆ
# Jennie Brain Module - LLM Orchestrator
#
# Roles:
# 1. Sentiment: News Sentiment Analysis (FAST Tier)
# 2. Hunter: Stock Analysis (REASONING Tier)
# 3. Judge: Final Decision (THINKING Tier)

import os
import logging
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Union

# [v6.0] Factory & Enum
from shared.llm_factory import LLMFactory, LLMTier
import shared.database as database
import shared.auth as auth

# [v6.0] Corrected Imports from shared modules
from shared.llm_prompts import (
    build_buy_prompt_mean_reversion,
    build_buy_prompt_golden_cross, # Used as build_buy_prompt_trend_following
    build_sell_prompt, # Used as build_sell_decision_prompt
    build_news_sentiment_prompt,
    build_debate_prompt,
    build_judge_prompt, # V4 Judge
    build_hunter_prompt_v5,
    build_judge_prompt_v5
)

from shared.llm_constants import ANALYSIS_RESPONSE_SCHEMA
# Alias for compatibility if needed, or just use ANALYSIS_RESPONSE_SCHEMA
JUDGE_RESPONSE_SCHEMA = ANALYSIS_RESPONSE_SCHEMA

# "youngs75_jennie.llm" ì´ë¦„ìœ¼ë¡œ ë¡œê±° ìƒì„±
logger = logging.getLogger(__name__)

class JennieBrain:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ 'BUY' ë˜ëŠ” 'SELL' ì‹ í˜¸ì— ëŒ€í•œ ìµœì¢… ê²°ìž¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    [v6.0] LLM Factory Pattern ë„ìž… - Hybrid Strategy (Local/Cloud)
    """
    
    def __init__(self, project_id=None, gemini_api_key_secret=None):
        # [v6.0] Factoryë¥¼ í†µí•´ ProviderëŠ” í•„ìš”í•  ë•Œ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        # ê¸°ì¡´ __init__ì—ì„œì˜ ë³µìž¡í•œ ì´ˆê¸°í™”ëŠ” ì œê±°í•˜ê³ , Factoryì— ìœ„ìž„í•©ë‹ˆë‹¤.
        logger.info("--- [JennieBrain] v6.0 Initialized (Factory Pattern) ---")
        
        # ë ˆê±°ì‹œ í˜¸í™˜ì„±ì„ ìœ„í•´ í•„ë“œë§Œ ë‚¨ê²¨ë‘  (ì‹¤ì œë¡œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        self.provider_gemini = None 
        self.provider_claude = None
        self.provider_openai = None

    def _get_provider(self, tier: LLMTier):
        """Helper to get provider from Factory with error handling"""
        try:
            return LLMFactory.get_provider(tier)
        except Exception as e:
            logger.error(f"âŒ [JennieBrain] Provider ë¡œë“œ ì‹¤íŒ¨ ({tier}): {e}")
            return None

    # -----------------------------------------------------------------
    # 'ì œë‹ˆ' ê²°ìž¬ ì‹¤í–‰
    # -----------------------------------------------------------------
    def get_jennies_decision(self, trade_type, stock_info, **kwargs):
        """
        LLMì„ í˜¸ì¶œí•˜ì—¬ ìµœì¢… ê²°ìž¬ë¥¼ ë°›ìŠµë‹ˆë‹¤.
        Trade Decision = Critical Task -> THINKING Tier
        """
        provider = self._get_provider(LLMTier.THINKING)
        if provider is None:
            return {"decision": "REJECT", "reason": "JennieBrain ì´ˆê¸°í™” ì‹¤íŒ¨ (Thinking Tier)", "quantity": 0}

        try:
            if trade_type == 'BUY_MR':
                buy_signal_type = kwargs.get('buy_signal_type', 'UNKNOWN')
                prompt = build_buy_prompt_mean_reversion(stock_info, buy_signal_type)
            elif trade_type == 'BUY_TREND':
                buy_signal_type = kwargs.get('buy_signal_type', 'GOLDEN_CROSS')
                # Use aliased function
                prompt = build_buy_prompt_golden_cross(stock_info, buy_signal_type)
            elif trade_type == 'SELL':
                market_status = kwargs.get('market_status', 'N/A') # build_sell_prompt expects stock_info mainly
                # build_sell_prompt signature: (stock_info). market_status implies prompt builder change?
                # Assuming build_sell_prompt only takes stock_info for now based on outline.
                prompt = build_sell_prompt(stock_info)
            else:
                return {"decision": "REJECT", "reason": "ì•Œ ìˆ˜ ì—†ëŠ” ê±°ëž˜ ìœ í˜•", "quantity": 0}

            logger.info(f"--- [JennieBrain] ê²°ìž¬ ìš”ì²­ ({trade_type}) via {provider.name} ---")
            
            # JSON ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
            DECISION_SCHEMA = {
                "type": "object",
                "properties": {
                    "decision": {"type": "string", "enum": ["APPROVE", "REJECT", "HOLD"]},
                    "reason": {"type": "string"},
                    "quantity": {"type": "integer"}
                },
                "required": ["decision", "reason", "quantity"]
            }

            result = provider.generate_json(
                prompt,
                DECISION_SCHEMA,
                temperature=0.1
            )
            
            logger.info(f"   ðŸ‘‘ ì œë‹ˆì˜ ê²°ìž¬: {result.get('decision')} ({result.get('reason')})")
            return result

        except Exception as e:
            logger.error(f"âŒ [JennieBrain] ê²°ìž¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"decision": "REJECT", "reason": f"System Error: {e}", "quantity": 0}

    # -----------------------------------------------------------------
    # ë‰´ìŠ¤ ê°ì„± ë¶„ì„
    # -----------------------------------------------------------------
    def analyze_news_sentiment(self, title, description):
        """
        ë‰´ìŠ¤ ì œëª©ê³¼ ìš”ì•½ì„ ë¶„ì„í•˜ì—¬ ê¸ì •/ë¶€ì • ì ìˆ˜ë¥¼ ë§¤ê¹ë‹ˆë‹¤.
        High Volume / Low Risk -> FAST Tier (Local LLM)
        """
        provider = self._get_provider(LLMTier.FAST)
        if provider is None:
             return {'score': 50, 'reason': 'ëª¨ë¸ ë¯¸ì´ˆê¸°í™” (ê¸°ë³¸ê°’)'}

        try:
            # build_news_sentiment_prompt args: news_title, news_summary
            prompt = build_news_sentiment_prompt(title, description)
            # logger.debug(f"--- [JennieBrain] ë‰´ìŠ¤ ë¶„ì„ via {provider.name} ---")
            
            result = provider.generate_json(
                prompt,
                ANALYSIS_RESPONSE_SCHEMA,
                temperature=0.0 # Deterministic
            )
            return result
        except Exception as e:
            logger.warning(f"âš ï¸ [News] Local LLM failed: {e}. Attempting Cloud Fallback...")
            try:
                # Fallback to THINKING Tier (Cloud)
                fallback_provider = self._get_provider(LLMTier.THINKING)
                result = fallback_provider.generate_json(
                    prompt,
                    ANALYSIS_RESPONSE_SCHEMA,
                    temperature=0.0
                )
                logger.info(f"   âœ… [News] Cloud Fallback Success via {fallback_provider.name}")
                return result
            except Exception as fb_e:
                logger.error(f"âŒ [News] Fallback failed: {fb_e}")
                return {'score': 50, 'reason': f'ë¶„ì„ ì‹¤íŒ¨ (Local+Cloud): {e}'}

    # -----------------------------------------------------------------
    # í† ë¡  (Bull vs Bear)
    # -----------------------------------------------------------------
    def run_debate_session(self, stock_info: dict, analysis_context: str = "", hunter_score: int = 0) -> str:
        """
        Bull vs Bear í† ë¡  ìƒì„± (Dynamic Role Allocation)
        Complex Creative Task -> REASONING Tier
        """
        provider = self._get_provider(LLMTier.REASONING)
        if provider is None:
             return "Debate Skipped (Model Error)"

        try:
            # [v6.0] Extract keywords for Dynamic Debate Context
            keywords = stock_info.get('dominant_keywords', [])
            
            # Pass hunter_score and keywords to build_debate_prompt
            prompt = build_debate_prompt(stock_info, hunter_score, keywords) 
            
            # [v6.0 Fix] generate_chat requires list of dicts, not str
            chat_history = [{"role": "user", "content": prompt}]
            
            logger.info(f"--- [JennieBrain/Debate] í† ë¡  ì‹œìž‘ via {provider.name} (HunterScore: {hunter_score}, KW: {keywords}) ---")
            
            result = provider.generate_chat(chat_history, temperature=0.7)
            # If result is dict (e.g. from structured output), extracting text. 
            # generate_chat usually returns dict with 'text' or json. 
            # But the caller expects str. 
            if isinstance(result, dict):
                return result.get('text') or result.get('content') or str(result)
            return str(result)

        except Exception as e:
            logger.warning(f"âš ï¸ [Debate] Local LLM failed: {e}. Attempting Cloud Fallback...")
            try:
                fallback_provider = self._get_provider(LLMTier.THINKING)
                if fallback_provider is None:
                    raise ValueError("Fallback provider (Thinking Tier) not available")

                logger.info(f"--- [JennieBrain/Debate] Cloud Fallback via {fallback_provider.name} ---")
                chat_history = [{"role": "user", "content": prompt}]
                result = fallback_provider.generate_chat(chat_history, temperature=0.7)
                
                if isinstance(result, dict):
                    return result.get('text') or result.get('content') or str(result)
                return str(result)
            except Exception as fb_e:
                logger.error(f"âŒ [Debate] Fallback failed: {fb_e}")
                return f"Debate Error: {e}"

    # -----------------------------------------------------------------
    # Check if stock exists (Legacy helper, optional)
    # -----------------------------------------------------------------
    def verify_parameter_change(self, stock_info: dict, param_name: str, old_val, new_val) -> dict:
        # Simple task -> FAST Tier
        provider = self._get_provider(LLMTier.FAST)
        if not provider: return {"authorized": False}
        return {"authorized": True, "reason": "Auto-approved by FAST tier"}

    # -----------------------------------------------------------------
    # [v4.0] Judge (Supreme Jennie) ìµœì¢… íŒê²°
    # -----------------------------------------------------------------
    def run_judge_scoring(self, stock_info: dict, debate_log: str) -> dict:
        """
        Judge Scoring = Critical Decision -> THINKING Tier
        """
        provider = self._get_provider(LLMTier.THINKING)
        if provider is None:
             return {'score': 0, 'grade': 'D', 'reason': 'Provider Error'}

        try:
            prompt = build_judge_prompt(stock_info, debate_log)
            logger.info(f"--- [JennieBrain/Judge] íŒê²° via {provider.name} ---")
            
            result = provider.generate_json(
                prompt,
                JUDGE_RESPONSE_SCHEMA, 
                temperature=0.1
            )
            return result
        except Exception as e:
            logger.error(f"âŒ [Judge] íŒê²° ì‹¤íŒ¨: {e}")
            return {'score': 0, 'grade': 'D', 'reason': f"ì˜¤ë¥˜: {e}"}

    # -----------------------------------------------------------------
    # [v1.0] Scout Hybrid Scoring
    # -----------------------------------------------------------------
    def get_jennies_analysis_score_v5(self, stock_info: dict, quant_context: str = None) -> dict:
        """
        v5 Hunter = Reasoning Task -> REASONING Tier
        """
        provider = self._get_provider(LLMTier.REASONING)
        if provider is None:
            return {'score': 0, 'grade': 'D', 'reason': 'Provider Error'}
        
        try:
            prompt = build_hunter_prompt_v5(stock_info, quant_context)
            logger.info(f"--- [JennieBrain/v5-Hunter] ë¶„ì„ via {provider.name} ---")
            
            result = provider.generate_json(
                prompt,
                ANALYSIS_RESPONSE_SCHEMA,
                temperature=0.2
            )
            logger.info(f"   âœ… v5 Hunter ì™„ë£Œ: {stock_info.get('name')} - {result.get('score')}ì ")
            return result
        except Exception as e:
            logger.warning(f"âš ï¸ [v5-Hunter] Local LLM failed: {e}. Attempting Cloud Fallback...")
            try:
                fallback_provider = self._get_provider(LLMTier.THINKING)
                if fallback_provider is None:
                    raise ValueError("Fallback provider (Thinking Tier) not available")

                logger.info(f"--- [JennieBrain/v5-Hunter] Cloud Fallback via {fallback_provider.name} ---")
                result = fallback_provider.generate_json(
                    prompt,
                    ANALYSIS_RESPONSE_SCHEMA,
                    temperature=0.2
                )
                return result
            except Exception as fb_e:
                logger.error(f"âŒ [v5-Hunter] Fallback failed: {fb_e}")
                return {'score': 0, 'grade': 'D', 'reason': f"ì˜¤ë¥˜(Local+Cloud): {e}"}

    def run_judge_scoring_v5(self, stock_info: dict, debate_log: str, quant_context: str = None) -> dict:
        """
        v5 Judge = Critical Decision -> THINKING Tier
        [Strategy Gate]: Hunter score < 70 (Grade B) will be auto-rejected to save Cloud costs and avoid weak signals.
        """
        # 1. Strategy Gate Check (Junho's Condition)
        hunter_score = stock_info.get('hunter_score', 0)
        # Default Threshold: 70 (B Grade)
        JUDGE_THRESHOLD = 70 
        
        if hunter_score < JUDGE_THRESHOLD:
            logger.info(f"ðŸš« [Gatekeeper] Judge Skipped. Hunter Score {hunter_score} < {JUDGE_THRESHOLD}. Auto-Reject.")
            return {
                'score': hunter_score, 
                'grade': 'D', 
                'reason': f"Hunter Score({hunter_score}) failed to meet Judge Threshold({JUDGE_THRESHOLD}). Auto-Rejected."
            }

        provider = self._get_provider(LLMTier.THINKING)
        if provider is None:
            return {'score': 0, 'grade': 'D', 'reason': 'Provider Error'}
            
        try:
            # 2. Structured Logging (Minji's Request)
            call_reason = "HighConviction_Verification"
            logger.info(json.dumps({
                "event": "ThinkingTier_Call",
                "tier": "THINKING",
                "task": "Judge_v5",
                "model": provider.client.__class__.__name__ if hasattr(provider, 'client') else provider.name,
                "reason": call_reason,
                "input_score": hunter_score
            }))
            
            prompt = build_judge_prompt_v5(stock_info, debate_log, quant_context)
            logger.info(f"--- [JennieBrain/v5-Judge] íŒê²° via {provider.name} (Why: {call_reason}) ---")
            
            result = provider.generate_json(
                prompt,
                ANALYSIS_RESPONSE_SCHEMA,
                temperature=0.1
            )
            return result
        except Exception as e:
            logger.error(f"âŒ [v5-Judge] ì˜¤ë¥˜: {e}")
            return {'score': 0, 'grade': 'D', 'reason': f"ì˜¤ë¥˜: {e}"}

    # -----------------------------------------------------------------
    # [New] Daily Briefing (Centralized from reporter.py)
    # -----------------------------------------------------------------
    def generate_daily_briefing(self, market_summary: str, execution_log: str) -> str:
        """
        Generate Daily Briefing Report.
        Task Type: REASONING or THINKING (depending on desired quality).
        Let's use THINKING for high quality report.
        """
        provider = self._get_provider(LLMTier.THINKING) 
        if provider is None:
            return "ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: ëª¨ë¸ ì´ˆê¸°í™” ì˜¤ë¥˜"

        prompt = f"""
        ë‹¹ì‹ ì€ í”„ë¡œíŽ˜ì…”ë„ ì£¼ì‹ íˆ¬ìž ë³´ê³ ì„œ ìž‘ì„±ìžìž…ë‹ˆë‹¤.
        ì˜¤ëŠ˜ì˜ ì‹œìž¥ ìƒí™©ê³¼ ìžë™ë§¤ë§¤ ìˆ˜í–‰ ë¡œê·¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ì¼ì¼ ë¸Œë¦¬í•‘ ë¦¬í¬íŠ¸'ë¥¼ ìž‘ì„±í•´ì£¼ì„¸ìš”.

        [ì‹œìž¥ ìš”ì•½]
        {market_summary}

        [ì˜¤ëŠ˜ì˜ ë§¤ë§¤ ìˆ˜í–‰ ë¡œê·¸]
        {execution_log}

        [ìž‘ì„± ê°€ì´ë“œ]
        1. í†¤ì•¤ë§¤ë„ˆ: ì „ë¬¸ì ì´ê³  ì‹ ë¢°ê° ìžˆê²Œ, ê·¸ëŸ¬ë‚˜ ê²©ë ¤í•˜ëŠ” ë§íˆ¬.
        2. êµ¬ì¡°: ì‹œìž¥ í˜„í™© -> ë§¤ë§¤ ì„±ê³¼ -> í–¥í›„ ì „ëžµ -> ë§ˆë¬´ë¦¬ ì¸ì‚¬.
        3. í…”ë ˆê·¸ëž¨ ë©”ì‹ ì €ìš©ìœ¼ë¡œ Markdown í¬ë§·ì„ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„± ì¢‹ê²Œ ìž‘ì„±.
        """
        try:
            logger.info(f"--- [JennieBrain/Briefing] ë¦¬í¬íŠ¸ ìƒì„± via {provider.name} ---")
            # [v6.0 Fix] generate_chat requires list of dicts
            chat_history = [{"role": "user", "content": prompt}]
            result = provider.generate_chat(chat_history, temperature=0.7)
            
            if isinstance(result, dict):
                return result.get('text') or result.get('content') or str(result)
            return str(result)
        except Exception as e:
            logger.error(f"âŒ [Briefing] ì‹¤íŒ¨: {e}")
            return "ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    # -----------------------------------------------------------------
    # Legacy V1 Methods (Placeholders to prevent import errors)
    # -----------------------------------------------------------------
    def detect_competitor_events(self, target_stock_code: str, target_stock_name: str, sector: str, recent_news: List[Dict]) -> dict:
        return {'competitor_events': [], 'total_benefit_score': 0, 'analysis_reason': 'Legacy method placeholder'}

    def get_beneficiary_recommendations(self, event_company: str, event_type: str, event_summary: str, sector: str) -> dict:
        return {'beneficiaries': [], 'top_pick': None, 'holding_period': 'N/A', 'risk_note': 'Legacy method placeholder'}
