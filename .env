
# Hybrid LLM Tier Configuration
# FAST: Local Qwen 2.5 3B (Sentiment / Fast Checks)
TIER_FAST_PROVIDER=ollama
LOCAL_MODEL_FAST=qwen2.5:3b

# REASONING: Local Qwen 2.5 14B (Hunter / Analysis)
TIER_REASONING_PROVIDER=ollama
LOCAL_MODEL_REASONING=qwen2.5:14b

# THINKING: Cloud OpenAI/Claude (Judge / Daily Briefing) -> High Quality
TIER_THINKING_PROVIDER=openai
LOCAL_MODEL_THINKING=deepseek-r1:32b # Fallback or Opt-in

# Ollama Host (Docker Network)
# If services are on the same network 'msa-network', use service name 'ollama'
OLLAMA_HOST=http://ollama:11434

# Global Switch (Optional, but good for clarity)
USE_LOCAL_LLM=true
