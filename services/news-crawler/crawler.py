#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# crawler_job.py
# Version: v9.1
# ì‘ì—… LLM: Claude Opus 4.5
# Crawler Job - Cloud Scheduler(HTTP)ì— ì˜í•´ 10ë¶„ë§ˆë‹¤ ì‹¤í–‰ë˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
# [v9.0] KOSPI 200 ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ (WatchList ì˜ì¡´ì„± ì œê±°)
# [v9.1] ê²½ìŸì‚¬ ìˆ˜í˜œ ë¶„ì„ ì—°ë™ (Claude Opus 4.5)

from concurrent.futures import ThreadPoolExecutor, as_completed
import time
# import chromadb  # Lazy importë¡œ ë³€ê²½ (ì´ˆê¸°í™” ì‹œê°„ ë‹¨ì¶•)
import sys
import json
import urllib.parse
import feedparser # type: ignore
import logging
import os 
import calendar
from dotenv import load_dotenv 
from datetime import datetime, timedelta, timezone

# [v9.0] FinanceDataReader for KOSPI 200 Universe
try:
    import FinanceDataReader as fdr
    FDR_AVAILABLE = True
except ImportError:
    FDR_AVAILABLE = False

# 'youngs75_jennie' íŒ¨í‚¤ì§€ë¥¼ ì°¾ê¸° ìœ„í•´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ í´ë”ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
# Dockerfileì—ì„œ /app/crawler_job.pyë¡œ ë³µì‚¬ë˜ë¯€ë¡œ, /appì´ í”„ë¡œì íŠ¸ ë£¨íŠ¸
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.append(PROJECT_ROOT)

# ==============================================================================
# ë¡œê±°(Logger) ì„¤ì •
# ==============================================================================
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

try:
    import shared.auth as auth
    import shared.database as database
    from shared.llm import JennieBrain # ê°ì„± ë¶„ì„ì„ ìœ„í•œ JennieBrain ì„í¬íŠ¸
    from shared.gemini import ensure_gemini_api_key
    # [v9.1] ê²½ìŸì‚¬ ìˆ˜í˜œ ë¶„ì„ ëª¨ë“ˆ
    from shared.news_classifier import NewsClassifier, get_classifier
    from shared.hybrid_scoring.competitor_analyzer import CompetitorAnalyzer
    logger.info("âœ… 'shared' íŒ¨í‚¤ì§€ ëª¨ë“ˆ import ì„±ê³µ")
except ImportError as e: # type: ignore
    logger.error(f"ğŸš¨ 'shared' ê³µìš© íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! (ì˜¤ë¥˜: {e})")
    auth = None
    database = None
    JennieBrain = None
    ensure_gemini_api_key = None
    NewsClassifier = None
    get_classifier = None
    CompetitorAnalyzer = None
except Exception as e:
    logger.error(f"ğŸš¨ 'shared' íŒ¨í‚¤ì§€ import ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
    auth = None
    database = None
    JennieBrain = None
    ensure_gemini_api_key = None
    NewsClassifier = None
    get_classifier = None
    CompetitorAnalyzer = None

from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ==============================================================================
# 1. ì „ì—­ ì„¤ì • (Constants)
# ==============================================================================

# Chroma ì„œë²„
CHROMA_SERVER_HOST = os.getenv("CHROMA_SERVER_HOST", "10.178.0.2") 
CHROMA_SERVER_PORT = 8000
COLLECTION_NAME = "rag_stock_data"

# RAG ì„¤ì •
DATA_TTL_DAYS = 7
VERTEX_AI_BATCH_SIZE = 10
MAX_SENTIMENT_DOCS_PER_RUN = int(os.getenv("MAX_SENTIMENT_DOCS_PER_RUN", "40"))
SENTIMENT_COOLDOWN_SECONDS = float(os.getenv("SENTIMENT_COOLDOWN_SECONDS", "0.2"))

# --- ğŸ”½ 'ì¼ë°˜ ê²½ì œ' RSS í”¼ë“œ ğŸ”½ ---
GENERAL_RSS_FEEDS = [
    {"source_name": "Maeil Business (Economy)", "url": "https://www.mk.co.kr/rss/50000001/"},
    {"source_name": "Maeil Business (Stock)", "url": "https://www.mk.co.kr/rss/50100001/"},
    {"source_name": "Investing.com (News)", "url": "https://kr.investing.com/rss/news.rss"}
]

# ==============================================================================
# LangChain, Chroma í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# ==============================================================================

# ==============================================================================
# ì „ì—­ ë³€ìˆ˜ (ì§€ì—° ì´ˆê¸°í™”)
# ==============================================================================

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ëª¨ë“ˆ ì„í¬íŠ¸ ì‹œ)
load_dotenv()

GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID")
DB_SERVICE_NAME = os.getenv("OCI_DB_SERVICE_NAME")
WALLET_DIR_NAME = os.getenv("OCI_WALLET_DIR_NAME", "wallet")
WALLET_PATH = os.path.join(PROJECT_ROOT, WALLET_DIR_NAME)

# ì§€ì—° ì´ˆê¸°í™”ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ (Noneìœ¼ë¡œ ì‹œì‘)
embeddings = None
text_splitter = None
db_client = None
vectorstore = None
jennie_brain = None # JennieBrain ì¸ìŠ¤í„´ìŠ¤

def initialize_services():
    """
    LangChain ë° ChromaDB ì„œë¹„ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    run_collection_job() ì‹¤í–‰ ì‹œì—ë§Œ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    global embeddings, text_splitter, db_client, vectorstore, jennie_brain
    
    logger.info("... [RAG Crawler v8.1] LangChain ë° AI ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹œì‘ ...")
    try:
        api_key = ensure_gemini_api_key()
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/gemini-embedding-001",
            google_api_key=api_key,
        )
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        logger.info("âœ… LangChain ì»´í¬ë„ŒíŠ¸(Embedding, Splitter) ì´ˆê¸°í™” ì„±ê³µ.")
        
        # JennieBrain ì´ˆê¸°í™” (ê°ì„± ë¶„ì„ìš©)
        try:
            jennie_brain = JennieBrain(
                project_id=GCP_PROJECT_ID,
                gemini_api_key_secret=os.getenv("SECRET_ID_GEMINI_API_KEY")
            )
            logger.info("âœ… JennieBrain (ê°ì„± ë¶„ì„ê¸°) ì´ˆê¸°í™” ì„±ê³µ.")
        except Exception as e:
            logger.warning(f"âš ï¸ JennieBrain ì´ˆê¸°í™” ì‹¤íŒ¨ (ê°ì„± ë¶„ì„ Skip): {e}")
            jennie_brain = None

    except Exception as e:
        logger.exception("ğŸ”¥ LangChain ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨!")
        raise
    
    logger.info(f"... [RAG Crawler v8.1] Chroma ì„œë²„ ({CHROMA_SERVER_HOST}:{CHROMA_SERVER_PORT}) ì—°ê²° ì‹œë„ ...")
    try:
        # Lazy import: chromadbëŠ” ì‹¤ì œ ì‚¬ìš© ì‹œì ì—ë§Œ import
        import chromadb
        
        db_client = chromadb.HttpClient(host=CHROMA_SERVER_HOST, port=CHROMA_SERVER_PORT)
        vectorstore = Chroma(client=db_client, collection_name=COLLECTION_NAME, embedding_function=embeddings)
        db_client.heartbeat() 
        logger.info(f"âœ… Chroma ì„œë²„ ({CHROMA_SERVER_HOST}) ì—°ê²° ì„±ê³µ!")
    except Exception as e:
        logger.exception(f"ğŸ”¥ Chroma ì„œë²„ ({CHROMA_SERVER_HOST}) ì—°ê²° ì‹¤íŒ¨!")
        raise

# ==============================================================================
# í•µì‹¬ í•¨ìˆ˜ ì •ì˜
# ==============================================================================

def get_kospi_200_universe():
    """
    [v9.0] KOSPI ì‹œê°€ì´ì•¡ ìƒìœ„ 200ê°œ ì¢…ëª©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    Scoutì™€ ë™ì¼í•œ Universeë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    universe_size = int(os.getenv("SCOUT_UNIVERSE_SIZE", "200"))
    logger.info(f"  (1/6) [v9.0] KOSPI ì‹œì´ ìƒìœ„ {universe_size}ê°œ ì¢…ëª© ë¡œë“œ ì¤‘...")
    
    # 1. FinanceDataReader ì‹œë„
    if FDR_AVAILABLE:
        try:
            logger.info("  (1/6) FinanceDataReaderë¡œ KOSPI ì¢…ëª© ì¡°íšŒ ì¤‘...")
            df = fdr.StockListing('KOSPI')
            
            if df is not None and not df.empty:
                # ì‹œê°€ì´ì•¡ ê¸°ì¤€ ì •ë ¬ (Marcap ë˜ëŠ” Market Cap ì»¬ëŸ¼)
                cap_col = None
                for col in ['Marcap', 'MarCap', 'Market Cap', 'marcap']:
                    if col in df.columns:
                        cap_col = col
                        break
                
                if cap_col:
                    df = df.sort_values(by=cap_col, ascending=False)
                
                # ìƒìœ„ Nê°œ ì¶”ì¶œ
                top_stocks = df.head(universe_size)
                
                # Code, Name ì»¬ëŸ¼ ì°¾ê¸°
                code_col = 'Code' if 'Code' in top_stocks.columns else 'Symbol'
                name_col = 'Name' if 'Name' in top_stocks.columns else 'name'
                
                universe = []
                for _, row in top_stocks.iterrows():
                    code = str(row.get(code_col, '')).zfill(6)
                    name = row.get(name_col, f'ì¢…ëª©_{code}')
                    if code and len(code) == 6:
                        universe.append({"code": code, "name": name})
                
                if universe:
                    logger.info(f"âœ… (1/6) FinanceDataReaderë¡œ {len(universe)}ê°œ ì¢…ëª© ë¡œë“œ ì™„ë£Œ!")
                    return universe
        except Exception as e:
            logger.warning(f"âš ï¸ (1/6) FinanceDataReader ì‹¤íŒ¨: {e}")
    
    # 2. Fallback: DBì˜ WatchList ì‚¬ìš©
    logger.info("  (1/6) Fallback: DB WatchList ì¡°íšŒ ì¤‘...")
    return get_watchlist_from_db()


def get_watchlist_from_db():
    """
    [v9.0] DBì—ì„œ WatchListë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (Fallbackìš©).
    """
    db_conn = None
    try:
        db_user = auth.get_secret(os.getenv("SECRET_ID_ORACLE_DB_USER"), GCP_PROJECT_ID)
        db_password = auth.get_secret(os.getenv("SECRET_ID_ORACLE_DB_PASSWORD"), GCP_PROJECT_ID)
        
        db_conn = database.get_db_connection(
            db_user=db_user,
            db_password=db_password,
            db_service_name=DB_SERVICE_NAME,
            wallet_path=WALLET_PATH
        )
        if not db_conn:
            logger.error("ğŸ”¥ (1/6) DB ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (Skip)")
            return []
 
        cursor = db_conn.cursor()
        sql = "SELECT stock_code, stock_name FROM WATCHLIST"
        cursor.execute(sql)
        
        watchlist = []
        for row in cursor.fetchall():
            watchlist.append({"code": row[0], "name": row[1]})
 
        logger.info(f"âœ… (1/6) 'WatchList' {len(watchlist)}ê°œ ë¡œë“œ ì„±ê³µ.")
        return watchlist
        
    except Exception as e:
        logger.exception(f"ğŸ”¥ (1/6) DB 'get_watchlist_from_db' í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ!")
        return []
    finally:
        if db_conn:
            db_conn.close()
            logger.info("... (1/6) DB ì—°ê²°ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

def get_numeric_timestamp(feed_entry):
    """
    feed_entryì—ì„œ 'ë°œí–‰ ì‹œê°„'ì„ UTC ê¸°ì¤€ ìˆ«ì íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    if hasattr(feed_entry, 'published_parsed') and feed_entry.published_parsed:
        try:
            return int(calendar.timegm(feed_entry.published_parsed))
        except Exception:
            return int(datetime.now(timezone.utc).timestamp())
    else:
        return int(datetime.now(timezone.utc).timestamp())

def crawl_news_for_stock(stock_code, stock_name):
    """
    Google News RSSë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì¢…ëª©ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    logger.info(f"  (2/6) [App 5] '{stock_name}({stock_code})' Google News RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
    documents = []
    try:
        query = f'"{stock_name}" OR "{stock_code}"'
        encoded_query = urllib.parse.quote(query)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
        feed = feedparser.parse(rss_url)
        
        if not feed.entries:
            logger.info(f"  (2/6) '{stock_name}' ê´€ë ¨ ì‹ ê·œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (Skip)")
            return []

        for entry in feed.entries:
            doc = Document(
                page_content=f"ë‰´ìŠ¤ ì œëª©: {entry.title}\në§í¬: {entry.link}",
                metadata={
                    "stock_code": stock_code,
                    "stock_name": stock_name,
                    "source": f"Google News RSS ({entry.get('source', {}).get('title', 'N/A')})",
                    "source_url": entry.link, 
                    "created_at_utc": get_numeric_timestamp(entry)
                }
            )
            documents.append(doc)
    except Exception as e:
        logger.exception(f"ğŸ”¥ (2/6) '{stock_name}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
    return documents

def crawl_general_news():
    """
    ë¯¸ë¦¬ ì •ì˜ëœ 'GENERAL_RSS_FEEDS' ëª©ë¡ì˜ ì¼ë°˜ ê²½ì œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    logger.info(f"  (3/6) [App 5] 'ì¼ë°˜ ê²½ì œ' RSS {len(GENERAL_RSS_FEEDS)}ê°œ í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
    documents = []
    
    for feed_info in GENERAL_RSS_FEEDS:
        source = feed_info["source_name"]
        url = feed_info["url"]
        logger.info(f"  (3/6) ... '{source}' ìˆ˜ì§‘ ì¤‘ ...")
        try:
            feed = feedparser.parse(url)
            if not feed.entries:
                logger.info(f"  (3/6) '{source}'ì— ì‹ ê·œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. (Skip)")
                continue

            for entry in feed.entries:
                doc = Document(
                    page_content=f"ë‰´ìŠ¤ ì œëª©: {entry.title}\në§í¬: {entry.link}",
                    metadata={
                        "source": source,
                        "source_url": entry.link, 
                        "created_at_utc": get_numeric_timestamp(entry)
                    }
                )
                documents.append(doc)
        except Exception as e:
            logger.exception(f"ğŸ”¥ (3/6) '{source}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
            
    logger.info(f"âœ… (3/6) 'ì¼ë°˜ ê²½ì œ' ë‰´ìŠ¤ ì´ {len(documents)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ.")
    return documents

def filter_new_documents(documents):
    """
    ChromaDBì— 'source_url'ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì—¬ ìƒˆë¡œìš´ ë¬¸ì„œë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    step_id = "(4/6)"
    logger.info(f"  {step_id} [App 5] ìˆ˜ì§‘ëœ ë¬¸ì„œ {len(documents)}ê°œ ì¼ê´„ ì¤‘ë³µ ê²€ì‚¬ ì‹œì‘...")
    if not documents:
        return []

    urls_to_check = list(set([doc.metadata["source_url"] for doc in documents if "source_url" in doc.metadata]))
    if not urls_to_check:
        return documents

    existing_results = vectorstore.get(where={"source_url": {"$in": urls_to_check}})
    existing_urls = set(item['source_url'] for item in existing_results.get('metadatas', []))
    new_docs = [doc for doc in documents if doc.metadata.get("source_url") not in existing_urls]

    logger.info(f"âœ… {step_id} ì¤‘ë³µ ê²€ì‚¬ ì™„ë£Œ. ìƒˆë¡œìš´ ë¬¸ì„œ {len(new_docs)}ê°œ ë°œê²¬.")
    return new_docs

def process_sentiment_analysis(documents):
    """
    [New] ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì¤‘ ì¢…ëª© ë‰´ìŠ¤ì— ëŒ€í•´ ì‹¤ì‹œê°„ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ë¶„ì„ ê²°ê³¼ëŠ” Redis ë° Oracle DBì— ì €ì¥ë©ë‹ˆë‹¤.
    """
    if not jennie_brain or not documents:
        return

    logger.info(f"  [Sentiment] ì‹ ê·œ ë¬¸ì„œ {len(documents)}ê°œì— ëŒ€í•œ ê°ì„± ë¶„ì„ ì‹œì‘...")
    
    # DB ì—°ê²° (ì €ì¥ìš©)
    db_conn = None
    try:
        db_user = auth.get_secret(os.getenv("SECRET_ID_ORACLE_DB_USER"), GCP_PROJECT_ID)
        db_password = auth.get_secret(os.getenv("SECRET_ID_ORACLE_DB_PASSWORD"), GCP_PROJECT_ID)
        db_conn = database.get_db_connection(db_user, db_password, DB_SERVICE_NAME, WALLET_PATH)
    except Exception as e:
        logger.error(f"âŒ [Sentiment] DB ì—°ê²° ì‹¤íŒ¨: {e}")

    processed_count = 0
    for doc in documents:
        if 0 < MAX_SENTIMENT_DOCS_PER_RUN <= processed_count:
            logger.info(
                "  [Sentiment] 1íšŒ ì‹¤í–‰ë‹¹ ë¶„ì„ ì œí•œ(%sê°œ)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ë‚˜ë¨¸ì§€ëŠ” ë‹¤ìŒ ì£¼ê¸°ì— ì²˜ë¦¬ë©ë‹ˆë‹¤.",
                MAX_SENTIMENT_DOCS_PER_RUN
            )
            break

        stock_code = doc.metadata.get("stock_code")
        # ì¢…ëª© ì½”ë“œê°€ ìˆëŠ” ë‰´ìŠ¤ë§Œ ë¶„ì„ (ì¼ë°˜ ê²½ì œ ë‰´ìŠ¤ëŠ” ì œì™¸)
        if not stock_code:
            continue
            
        title = doc.metadata.get("source", "ì œëª© ì—†ìŒ").replace("Google News RSS", "") # ë©”íƒ€ë°ì´í„° êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”. 
        # ìœ„ í¬ë¡¤ë§ ë¡œì§ì„ ë³´ë©´ metadata['source']ëŠ” ì¶œì²˜ëª…ì´ê³ , ì œëª©ì€ page_contentì— ìˆìŒ.
        # page_content íŒŒì‹± í•„ìš”: "ë‰´ìŠ¤ ì œëª©: {title}\në§í¬: {link}"
        content_lines = doc.page_content.split('\n')
        news_title = content_lines[0].replace("ë‰´ìŠ¤ ì œëª©: ", "") if len(content_lines) > 0 else "ì œëª© ì—†ìŒ"
        news_link = doc.metadata.get("source_url")
        published_at = doc.metadata.get("created_at_utc")

        # 1. LLM ê°ì„± ë¶„ì„
        try:
            result = jennie_brain.analyze_news_sentiment(news_title, news_title)
            score = result.get('score', 50)
            reason = result.get('reason', 'ë¶„ì„ ë¶ˆê°€')
        except Exception as e:
            logger.warning(f"âš ï¸ [Sentiment] ë¶„ì„ ì¤‘ ì˜¤ë¥˜ (Skip): {e}")
            continue

        # 2. Redis ì €ì¥ (Fast Handsìš©)
        try:
            database.set_sentiment_score(stock_code, score, reason)
        except Exception as e:
            logger.warning(f"âš ï¸ [Sentiment] Redis ì €ì¥ ì‹¤íŒ¨ (Skip): {e}")
            continue
        
        # 3. Oracle DB ì €ì¥ (ê¸°ë¡ìš©)
        if db_conn:
            try:
                database.save_news_sentiment(db_conn, stock_code, news_title, score, reason, news_link, published_at)
            except Exception as e:
                logger.warning(f"âš ï¸ [Sentiment] DB ì €ì¥ ì‹¤íŒ¨ (Skip): {e}")
                continue
        
        processed_count += 1
        
        if SENTIMENT_COOLDOWN_SECONDS > 0:
            time.sleep(SENTIMENT_COOLDOWN_SECONDS)
            
    if db_conn:
        db_conn.close()
        
    logger.info(f"âœ… [Sentiment] ì¢…ëª© ë‰´ìŠ¤ {processed_count}ê±´ ê°ì„± ë¶„ì„ ë° ì €ì¥ ì™„ë£Œ.")


def process_competitor_benefit_analysis(documents):
    """
    [v9.1] ë‰´ìŠ¤ì—ì„œ ê²½ìŸì‚¬ ìˆ˜í˜œ ê¸°íšŒë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    ì•…ì¬(ë³´ì•ˆì‚¬ê³ , ë¦¬ì½œ, ì˜¤ë„ˆë¦¬ìŠ¤í¬ ë“±) ë°œìƒ ì‹œ:
    1. í•´ë‹¹ ì¢…ëª©ì˜ ê²½ìŸì‚¬ë“¤ì„ ì¡°íšŒ
    2. ìˆ˜í˜œ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ Redisì— ì €ì¥
    3. DBì— ì´ë²¤íŠ¸ ê¸°ë¡
    """
    if not get_classifier or not CompetitorAnalyzer or not documents:
        return
    
    logger.info(f"  [ê²½ìŸì‚¬ ìˆ˜í˜œ] ì‹ ê·œ ë¬¸ì„œ {len(documents)}ê°œ ê²½ìŸì‚¬ ìˆ˜í˜œ ë¶„ì„ ì‹œì‘...")
    
    # ëª¨ë“ˆ ì´ˆê¸°í™”
    classifier = get_classifier()
    competitor_analyzer = CompetitorAnalyzer()
    
    # DB ì—°ê²° (SQLAlchemy)
    from shared.db.connection import init_engine, get_session
    from shared.db.models import IndustryCompetitors, CompetitorBenefitEvents
    from datetime import timedelta
    
    try:
        init_engine(None, None, None, None)
        session = get_session()
    except Exception as e:
        logger.error(f"âŒ [ê²½ìŸì‚¬ ìˆ˜í˜œ] DB ì—°ê²° ì‹¤íŒ¨: {e}")
        return
    
    benefit_events_created = 0
    
    for doc in documents:
        stock_code = doc.metadata.get("stock_code")
        if not stock_code:
            continue
        
        # ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ
        content_lines = doc.page_content.split('\n')
        news_title = content_lines[0].replace("ë‰´ìŠ¤ ì œëª©: ", "") if len(content_lines) > 0 else ""
        news_link = doc.metadata.get("source_url")
        
        # 1. ë‰´ìŠ¤ ë¶„ë¥˜
        classification = classifier.classify(news_title)
        if not classification:
            continue
        
        # 2. ì•…ì¬ì¸ì§€ í™•ì¸ (ê²½ìŸì‚¬ ìˆ˜í˜œê°€ ìˆëŠ” ì¹´í…Œê³ ë¦¬ë§Œ)
        if classification.sentiment != 'NEGATIVE' or classification.competitor_benefit <= 0:
            continue
        
        logger.info(f"  ğŸ”´ [ì•…ì¬ ê°ì§€] {stock_code} - {classification.category}: {news_title[:50]}...")
        
        # 3. í•´ë‹¹ ì¢…ëª©ì˜ ì„¹í„° ë° ê²½ìŸì‚¬ ì¡°íšŒ
        affected_stock = session.query(IndustryCompetitors).filter(
            IndustryCompetitors.stock_code == stock_code
        ).first()
        
        if not affected_stock:
            logger.debug(f"     â†’ {stock_code}ëŠ” ê²½ìŸì‚¬ ë§¤í•‘ì— ì—†ìŒ (Skip)")
            continue
        
        sector_code = affected_stock.sector_code
        sector_name = affected_stock.sector_name
        affected_name = affected_stock.stock_name
        
        # 4. ë™ì¼ ì„¹í„° ê²½ìŸì‚¬ ì¡°íšŒ
        competitors = session.query(IndustryCompetitors).filter(
            IndustryCompetitors.sector_code == sector_code,
            IndustryCompetitors.stock_code != stock_code,
            IndustryCompetitors.is_active == 1
        ).all()
        
        if not competitors:
            logger.debug(f"     â†’ {sector_name} ì„¹í„°ì— ê²½ìŸì‚¬ ì—†ìŒ (Skip)")
            continue
        
        # 5. ê° ê²½ìŸì‚¬ì— ëŒ€í•´ ìˆ˜í˜œ ì´ë²¤íŠ¸ ìƒì„±
        expires_at = datetime.now(timezone.utc) + timedelta(days=classification.duration_days)
        
        for competitor in competitors:
            # ê¸°ì¡´ ë™ì¼ ì´ë²¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸ (24ì‹œê°„ ë‚´ ì¤‘ë³µ ë°©ì§€)
            existing = session.query(CompetitorBenefitEvents).filter(
                CompetitorBenefitEvents.affected_stock_code == stock_code,
                CompetitorBenefitEvents.beneficiary_stock_code == competitor.stock_code,
                CompetitorBenefitEvents.event_type == classification.category,
                CompetitorBenefitEvents.detected_at >= datetime.now(timezone.utc) - timedelta(hours=24)
            ).first()
            
            if existing:
                logger.debug(f"     â†’ {competitor.stock_name} ì´ë¯¸ ì´ë²¤íŠ¸ ì¡´ì¬ (Skip)")
                continue
            
            # ìˆ˜í˜œ ì´ë²¤íŠ¸ ìƒì„±
            benefit_event = CompetitorBenefitEvents(
                affected_stock_code=stock_code,
                affected_stock_name=affected_name,
                event_type=classification.category,
                event_title=news_title[:1000],
                event_severity=classification.base_score,
                source_url=news_link,
                beneficiary_stock_code=competitor.stock_code,
                beneficiary_stock_name=competitor.stock_name,
                benefit_score=classification.competitor_benefit,
                sector_code=sector_code,
                sector_name=sector_name,
                status='ACTIVE',
                expires_at=expires_at
            )
            session.add(benefit_event)
            benefit_events_created += 1
            
            logger.info(
                f"  âœ… [ìˆ˜í˜œ ë“±ë¡] {competitor.stock_name}({competitor.stock_code}) "
                f"+{classification.competitor_benefit}ì  â† {affected_name} {classification.category}"
            )
            
            # 6. Redisì— ìˆ˜í˜œ ì ìˆ˜ ì €ì¥ (Scout Jobì—ì„œ í™œìš©)
            try:
                database.set_competitor_benefit_score(
                    stock_code=competitor.stock_code,
                    score=classification.competitor_benefit,
                    reason=f"ê²½ìŸì‚¬ {affected_name}ì˜ {classification.category}ë¡œ ì¸í•œ ìˆ˜í˜œ",
                    affected_stock=stock_code,
                    event_type=classification.category,
                    ttl=classification.duration_days * 86400
                )
            except Exception as e:
                logger.warning(f"âš ï¸ [ê²½ìŸì‚¬ ìˆ˜í˜œ] Redis ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # ì»¤ë°‹
    try:
        session.commit()
        logger.info(f"âœ… [ê²½ìŸì‚¬ ìˆ˜í˜œ] ìˆ˜í˜œ ì´ë²¤íŠ¸ {benefit_events_created}ê±´ ìƒì„± ì™„ë£Œ")
    except Exception as e:
        session.rollback()
        logger.error(f"âŒ [ê²½ìŸì‚¬ ìˆ˜í˜œ] DB ì»¤ë°‹ ì‹¤íŒ¨: {e}")
    finally:
        session.close()


def add_documents_to_chroma(documents):
    """
    ìƒˆë¡œìš´ Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶„í• (Chunking) í›„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    step_id = "(5/6)"
    if not documents:
        logger.info(f"  {step_id} [App 5] Chromaì— ì €ì¥í•  ìƒˆë¡œìš´ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. (Skip Write)")
        return

    logger.info(f"  {step_id} [App 5] 'ìƒˆ' ë¬¸ì„œ {len(documents)}ê°œ í…ìŠ¤íŠ¸ ë¶„í•  ë° ì„ë² ë”© ì¤‘...")
    try:
        splitted_docs = text_splitter.split_documents(documents)
        
        for i in range(0, len(splitted_docs), VERTEX_AI_BATCH_SIZE): # type: ignore
            batch_docs = splitted_docs[i : i + VERTEX_AI_BATCH_SIZE]
            logger.info(f"  {step_id} [App 4] 'ìƒˆ' ì²­í¬ {i+1} ~ {i+len(batch_docs)}ë²ˆ (ì´ {len(batch_docs)}ê°œ) ì €ì¥ ì‹œë„...")
            vectorstore.add_documents(
                batch_docs
            )
        
        logger.info(f"âœ… {step_id} [App 4] Chroma ì„œë²„ì— 'ìƒˆ' ì²­í¬ ì´ {len(splitted_docs)}ê°œ ì €ì¥ ì™„ë£Œ!")
    except Exception as e:
        logger.exception(f"ğŸ”¥ {step_id} [App 4] Chroma ì„œë²„ì— 'Write' ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ")

def cleanup_old_data_job():
    """
    DATA_TTL_DAYS(7ì¼)ê°€ ì§€ë‚œ ì˜¤ë˜ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ChromaDBì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    logger.info(f"\n[ë°ì´í„° ì •ë¦¬] {DATA_TTL_DAYS}ì¼ ê²½ê³¼í•œ ì˜¤ë˜ëœ RAG ë°ì´í„° ì‚­ì œ ì‹œì‘...")
    try:
        ttl_limit_timestamp = int((datetime.now(timezone.utc) - timedelta(days=DATA_TTL_DAYS)).timestamp())
        collection = vectorstore._collection
        
        logger.info(f"... [ë°ì´í„° ì •ë¦¬] created_at_utc < {ttl_limit_timestamp} ë°ì´í„° ì‚­ì œ ì¤‘ ...")
        collection.delete(where={"created_at_utc": {"$lt": ttl_limit_timestamp}})
        
        logger.info("âœ… [ë°ì´í„° ì •ë¦¬] ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ ì™„ë£Œ.")
    except Exception as e:
        logger.warning(f"âš ï¸ [ë°ì´í„° ì •ë¦¬] ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ==============================================================================
# ë©”ì¸ ì‘ì—… ì‹¤í–‰ í•¨ìˆ˜
# ==============================================================================

def run_collection_job():
    """
    ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì €ì¥ì„ ìœ„í•œ ë©”ì¸ íƒœìŠ¤í¬.
    ì´ í•¨ìˆ˜ê°€ ìŠ¤í¬ë¦½íŠ¸ì˜ 'ì§„ì…ì (Entrypoint)'ì´ ë©ë‹ˆë‹¤.
    [v9.0] KOSPI 200 ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ (Scout Universeì™€ ë™ì¼)
    """
    logger.info(f"\n--- [RAG ìˆ˜ì§‘ ë´‡ v9.0] ì‘ì—… ì‹œì‘ ---")
    
    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ì§€ì—° ì´ˆê¸°í™”)
    try:
        initialize_services()
    except Exception as e:
        logger.error(f"ğŸ”¥ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    try:
        all_fetched_documents = []

        # 1. 'ì¼ë°˜ ê²½ì œ' RSS ìˆ˜ì§‘
        general_news_docs = crawl_general_news()
        all_fetched_documents.extend(general_news_docs)

        # 2. [v9.0] KOSPI 200 Universe ë¡œë“œ (Scoutì™€ ë™ì¼)
        universe = get_kospi_200_universe()
        logger.info(f"  (2/6) [v9.0] KOSPI Universe {len(universe)}ê°œ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")

        # 3. ê° ì¢…ëª©ë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_stock = {executor.submit(crawl_news_for_stock, stock["code"], stock["name"]): stock for stock in universe}
            for future in as_completed(future_to_stock):
                stock = future_to_stock[future]
                try:
                    fetched_docs = future.result()
                    all_fetched_documents.extend(fetched_docs)
                except Exception as exc:
                    logger.error(f"ğŸ”¥ '{stock['name']}' ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤ë ˆë“œì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {exc}")

        # 4. 'ìƒˆë¡œìš´' ë¬¸ì„œë§Œ í•„í„°ë§ (Deduplication)
        new_documents_to_add = filter_new_documents(all_fetched_documents)
        
        # [New] 4-1. ìƒˆë¡œìš´ ë¬¸ì„œ ê°ì„± ë¶„ì„ ë° ì €ì¥
        process_sentiment_analysis(new_documents_to_add)
        
        # [v9.1] 4-2. ê²½ìŸì‚¬ ìˆ˜í˜œ ë¶„ì„ ë° ì €ì¥
        process_competitor_benefit_analysis(new_documents_to_add)
        
        # 5. 'ìƒˆë¡œìš´' ë¬¸ì„œë§Œ Chroma ì„œë²„ì— ì €ì¥ (Write)
        add_documents_to_chroma(new_documents_to_add)
        
        # 6. ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
        cleanup_old_data_job()
        
        logger.info(f"--- [RAG ìˆ˜ì§‘ ë´‡ v9.1] ì‘ì—… ì™„ë£Œ ---")
        
    except Exception as e:
        logger.exception(f"ğŸ”¥ [RAG ìˆ˜ì§‘ ë´‡ v9.0] ë©”ì¸ ì‘ì—… ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ")

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# =============================================================================

if __name__ == "__main__":
    
    start_time = time.time()

    # ë©”ì¸ ì‘ì—… ì‹¤í–‰
    try:
        run_collection_job()
    except Exception as e:
        logger.critical(f"âŒ [RAG Crawler v8.1] 'run_collection_job' ì‹¤í–‰ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
        
    end_time = time.time()
    logger.info(f"--- [RAG ìˆ˜ì§‘ ë´‡ v8.1] ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ (ì´ ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ) ---")
