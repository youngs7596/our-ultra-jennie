#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# services/news-crawler/main.py
# Version: v3.5
"""
News Crawler Service - Main API
Cloud Scheduler + RabbitMQ Self-Reschedule í•˜ì´ë¸Œë¦¬ë“œ íŒ¨í„´ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

import logging
import os
import uuid
from datetime import datetime, timezone
from flask import Flask, jsonify

from crawler import run_collection_job
from shared.rabbitmq import RabbitMQPublisher, RabbitMQWorker
from shared.scheduler_runtime import parse_job_message, SchedulerJobMessage
from shared.scheduler_client import mark_job_run

app = Flask(__name__)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
)
logger = logging.getLogger(__name__)

scheduler_job_worker = None
scheduler_job_publisher = None


def _run_crawler(trigger_source: str = "manual") -> dict:
    logger.info("ğŸ“° News Crawler ì‘ì—… ì‹œì‘ (trigger=%s)", trigger_source)
    run_collection_job()
    logger.info("âœ… News Crawler ì‘ì—… ì™„ë£Œ (trigger=%s)", trigger_source)
    return {"status": "success", "trigger": trigger_source}


def _get_amqp_url() -> str:
    return os.getenv("RABBITMQ_URL", "amqp://guest:guest@rabbitmq:5672/")


def _get_job_queue_name() -> str:
    scope = os.getenv("SCHEDULER_SCOPE", "real")
    default_queue = f"{scope}.jobs.news-crawler"
    return os.getenv("SCHEDULER_QUEUE_NEWS_CRAWLER", default_queue)


def _get_scheduler_job_id() -> str:
    return os.getenv("SCHEDULER_NEWS_CRAWLER_JOB_ID", "news-crawler")


def handle_scheduler_job(payload: dict):
    job_msg = parse_job_message(payload)
    # [v1.0] "unknown"ì¼ ë•Œë„ í™˜ê²½ë³€ìˆ˜ job_id ì‚¬ìš©
    effective_job_id = job_msg.job_id if job_msg.job_id and job_msg.job_id != "unknown" else _get_scheduler_job_id()
    logger.info(
        "ğŸ•’ News Crawler Scheduler Job ìˆ˜ì‹ : job=%s (effective=%s) run=%s delay=%s",
        job_msg.job_id,
        effective_job_id,
        job_msg.run_id,
        job_msg.next_delay_sec,
    )
    try:
        _run_crawler(trigger_source=f"scheduler/{job_msg.trigger_source}")
    except Exception as exc:
        logger.error("âŒ Scheduler Job ì‹¤íŒ¨: %s", exc, exc_info=True)
    finally:
        mark_job_run(effective_job_id, scope=job_msg.scope)


def start_scheduler_worker():
    global scheduler_job_worker, scheduler_job_publisher
    if os.getenv("ENABLE_NEWS_CRAWLER_JOB_WORKER", "true").lower() != "true":
        logger.info("âš ï¸ News Crawler Scheduler Worker ë¹„í™œì„±í™” ìƒíƒœ")
        return

    queue_name = _get_job_queue_name()
    scheduler_job_publisher = RabbitMQPublisher(
        amqp_url=_get_amqp_url(),
        queue_name=queue_name,
    )
    scheduler_job_worker = RabbitMQWorker(
        amqp_url=_get_amqp_url(),
        queue_name=queue_name,
        handler=handle_scheduler_job,
    )
    scheduler_job_worker.start()
    logger.info("âœ… News Crawler Scheduler Worker ì‹œì‘ (queue=%s)", queue_name)
    _bootstrap_scheduler_job()


def _bootstrap_scheduler_job():
    if not scheduler_job_publisher:
        logger.warning("âš ï¸ Scheduler Job Publisher ì—†ìŒ. Bootstrapì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    payload = {
        "job_id": _get_scheduler_job_id(),
        "scope": os.getenv("SCHEDULER_SCOPE", "real"),
        "run_id": str(uuid.uuid4()),
        "trigger_source": "startup_oneshot",
        "params": {},
        "timeout_sec": 300,
        "retry_limit": 1,
        "queued_at": datetime.now(timezone.utc).isoformat(),
    }

    message_id = scheduler_job_publisher.publish(payload)
    if message_id:
        logger.info("ğŸš€ News Crawler Startup Job ë°œí–‰ (message=%s)", message_id)
    else:
        logger.error("âŒ News Crawler Startup Job ë°œí–‰ ì‹¤íŒ¨")


# Gunicorn í™˜ê²½ì—ì„œë„ ì¦‰ì‹œ Scheduler Workerë¥¼ ë„ìš´ë‹¤.
start_scheduler_worker()


@app.route("/health", methods=["GET"])
def health():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return jsonify({"status": "healthy", "service": "news-crawler"}), 200


@app.route("/crawl", methods=["POST"])
def crawl():
    """
    ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤í–‰ ì—”ë“œí¬ì¸íŠ¸ (HTTP ìˆ˜ë™ íŠ¸ë¦¬ê±°)
    """
    try:
        result = _run_crawler(trigger_source="http")
        return jsonify(result), 200
    except Exception as e:
        logger.error(f"âŒ News Crawler ì‘ì—… ì‹¤íŒ¨: {e}", exc_info=True)
        return jsonify({"status": "error", "message": str(e)}), 500


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8080))
    app.run(host="0.0.0.0", port=port)

