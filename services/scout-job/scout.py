#!/usr/bin/env python3
# Version: v1.0
# ÏûëÏóÖ LLM: Claude Sonnet 4.5, Claude Opus 4.5
"""
Scout Job v1.0 - Ï¢ÖÎ™© Î∞úÍµ¥ ÌååÏù¥ÌîÑÎùºÏù∏
- ÍπêÍπêÌïú ÌïÑÌÑ∞ÎßÅ (Í∏∞Î≥∏Ï†êÏàò 20, Hunter ÌÜµÍ≥º 60Ï†ê, Judge ÏäπÏù∏ 75Ï†ê)
- [v1.0] ÏøºÌÑ∞Ï†ú ÎèÑÏûÖ: ÏµúÏ¢Ö Watchlist ÏÉÅÏúÑ 15Í∞úÎßå Ï†ÄÏû•
- [v1.0] Debate ÌîÑÎ°¨ÌîÑÌä∏ Í∞ïÌôî: Bull/Bear Ï∫êÎ¶≠ÌÑ∞ Í∑πÎã®Ï†ÅÏúºÎ°ú ÏÑ§Ï†ï
- Redis ÏÉÅÌÉú Ï†ÄÏû•: DashboardÏóêÏÑú Ïã§ÏãúÍ∞Ñ ÌååÏù¥ÌîÑÎùºÏù∏ ÏßÑÌñâ ÏÉÅÌô© ÌôïÏù∏ Í∞ÄÎä•
- Í≤ΩÏüÅÏÇ¨ ÏàòÌòú Ï†êÏàò Î∞òÏòÅ: Í≤ΩÏüÅÏÇ¨ ÏïÖÏû¨ Ïãú Hunter Ï†êÏàòÏóê Í∞ÄÏÇ∞
"""

import logging
import os
import sys
import time
import re
import threading
import json
import hashlib
from typing import Dict, Tuple, List, Optional
from datetime import datetime, timezone, timedelta
from dotenv import load_dotenv
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import redis

# Î°úÍπÖ ÏÑ§Ï†ïÏùÑ Î™®Îì† import Î≥¥Îã§ Î®ºÏ†Ä ÏàòÌñâ
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s] - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)

logger = logging.getLogger(__name__)

# Í≥µÏö© ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏Î•º ÏúÑÌïú Í≤ΩÎ°ú ÏÑ§Ï†ï
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))  # /app
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

import shared.auth as auth
import shared.database as database
from shared.db.connection import session_scope, ensure_engine_initialized
from shared.kis import KISClient as KIS_API
from shared.kis.gateway_client import KISGatewayClient
from shared.llm import JennieBrain
from shared.financial_data_collector import batch_update_watchlist_financial_data
from shared.gemini import ensure_gemini_api_key  # [v3.0] Local Gemini Auth Ï∂îÍ∞Ä
from shared.archivist import Archivist  # [v6.0] Data Strategy Logger

import chromadb
from langchain_chroma import Chroma
# from langchain_google_vertexai import VertexAIEmbeddings # [v3.0] Vertex AI Ï†úÍ±∞
from langchain_google_genai import GoogleGenerativeAIEmbeddings # [v3.0] Gemini API Key Í∏∞Î∞ò

# [v3.8] FinanceDataReader for KOSPI 200 Universe
try:
    import FinanceDataReader as fdr
    FDR_AVAILABLE = True
    logger.info("‚úÖ FinanceDataReader Î™®Îìà Î°úÎìú ÏÑ±Í≥µ")
except ImportError:
    FDR_AVAILABLE = False
    logger.warning("‚ö†Ô∏è FinanceDataReader ÎØ∏ÏÑ§Ïπò - ÎÑ§Ïù¥Î≤Ñ Í∏àÏúµ Ïä§ÌÅ¨ÎûòÌïëÏúºÎ°ú Ìè¥Î∞±")

# [v2.2 ÏàòÏ†ï] backtest Î™®Îìà ÏûÑÌè¨Ìä∏
try:
    from utilities.backtest import Backtester
    logger.info("‚úÖ Backtester Î™®Îìà ÏûÑÌè¨Ìä∏ ÏÑ±Í≥µ")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Backtester Î™®Îìà ÏûÑÌè¨Ìä∏ Ïã§Ìå® (Î∞±ÌÖåÏä§Ìä∏ Í∏∞Îä• ÎπÑÌôúÏÑ±Ìôî): {e}")
    Backtester = None

# Chroma ÏÑúÎ≤Ñ
CHROMA_SERVER_HOST = os.getenv("CHROMA_SERVER_HOST", "10.178.0.2") 
CHROMA_SERVER_PORT = 8000

# --- (B) Ï†ïÏ†Å Ïö∞ÎüâÏ£º Î™©Î°ù (ÏïàÏ†ÑÎßù/Fallback) ---
BLUE_CHIP_STOCKS = [
    {"code": "0001", "name": "KOSPI", "is_tradable": False},
    {"code": "005930", "name": "ÏÇºÏÑ±Ï†ÑÏûê", "is_tradable": True},
    # ... (Ïù¥Ìïò ÏÉùÎûµ, Í∏∞Ï°¥ Î¶¨Ïä§Ìä∏ Ïú†ÏßÄ)
    {"code": "000660", "name": "SKÌïòÏù¥ÎãâÏä§", "is_tradable": True},
    {"code": "035420", "name": "NAVER", "is_tradable": True},
    {"code": "035720", "name": "Ïπ¥Ïπ¥Ïò§", "is_tradable": True},
]

# =============================================================================
# [v1.1 Refactored] Ï∫êÏãú/ÏÉÅÌÉú Í¥ÄÎ¶¨ Ìï®ÏàòÎì§ÏùÄ scout_cache.pyÎ°ú Î∂ÑÎ¶¨Îê®
# =============================================================================
from scout_cache import (
    # ÏÉÅÏàò
    STATE_PREFIX, CANDIDATE_DIGEST_SUFFIX, CANDIDATE_HASHES_SUFFIX,
    LLM_CACHE_SUFFIX, LLM_LAST_RUN_SUFFIX, ISO_FORMAT_Z,
    REDIS_URL,
    # Redis Ìï®Ïàò
    _get_redis, _utcnow, update_pipeline_status, save_pipeline_results,
    # CONFIG ÌÖåÏù¥Î∏î Ìï®Ïàò
    _get_scope, _make_state_key, _load_json_config, _save_json_config,
    _get_last_llm_run_at, _save_last_llm_run_at,
    _load_candidate_state, _save_candidate_state,
    _load_llm_cache, _save_llm_cache,
    # LLM_EVAL_CACHE ÌÖåÏù¥Î∏î Ìï®Ïàò
    _load_llm_cache_from_db, _save_llm_cache_to_db, _save_llm_cache_batch,
    # Ï∫êÏãú Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨ Î∞è Ìï¥Ïãú Í≥ÑÏÇ∞
    _is_cache_valid_direct, _get_price_bucket, _get_volume_bucket, _get_foreign_direction,
    _hash_candidate_payload, _compute_candidate_hashes,
    _minutes_since, _parse_int_env, _is_cache_entry_valid,
    _record_to_watchlist_entry, _record_to_cache_payload, _cache_payload_to_record,
)

# =============================================================================
# [v1.1 Refactored] Ï¢ÖÎ™© Ïú†ÎãàÎ≤ÑÏä§ Í¥ÄÎ†® Ìï®ÏàòÎì§ÏùÄ scout_universe.pyÎ°ú Î∂ÑÎ¶¨Îê®
# =============================================================================
from scout_universe import (
    SECTOR_MAPPING, BLUE_CHIP_STOCKS, FDR_AVAILABLE,
    analyze_sector_momentum, get_hot_sector_stocks,
    get_dynamic_blue_chips, get_momentum_stocks,
)

# =============================================================================
# [v1.1 Refactored] ÏûêÎèô ÏµúÏ†ÅÌôî Ìï®ÏàòÎì§ÏùÄ scout_optimizer.pyÎ°ú Î∂ÑÎ¶¨Îê®
# =============================================================================
from scout_optimizer import (
    run_auto_parameter_optimization,
    run_simple_backtest, generate_optimized_params, verify_params_with_llm,
)

# =============================================================================
# [v1.1 Refactored] ÌååÏù¥ÌîÑÎùºÏù∏ ÌÉúÏä§ÌÅ¨ Ìï®ÏàòÎì§ÏùÄ scout_pipeline.pyÎ°ú Î∂ÑÎ¶¨Îê®
# =============================================================================
from scout_pipeline import (
    is_hybrid_scoring_enabled,
    process_quant_scoring_task,
    process_phase1_hunter_v5_task, process_phase23_judge_v5_task,
    process_phase1_hunter_task, process_phase23_debate_judge_task,
    process_llm_decision_task, fetch_kis_data_task,
)

_redis_client = None  # scout_cacheÏóêÏÑú Í¥ÄÎ¶¨ÌïòÏßÄÎßå Ìò∏ÌôòÏÑ± Ïú†ÏßÄ




def prefetch_all_data(candidate_stocks: Dict[str, Dict], kis_api, vectorstore) -> Tuple[Dict[str, Dict], Dict[str, str]]:
    """
    [v4.2] Phase 1 ÏãúÏûë Ï†ÑÏóê Î™®Îì† Îç∞Ïù¥ÌÑ∞Î•º ÏùºÍ¥Ñ Ï°∞ÌöåÌïòÏó¨ Ï∫êÏãú
    
    Returns:
        (snapshot_cache, news_cache) - Ï¢ÖÎ™©ÏΩîÎìúÎ•º ÌÇ§Î°ú ÌïòÎäî dict
    
    Ìö®Í≥º: Î≥ëÎ†¨ Ïä§Î†àÎìú ÏïàÏóêÏÑú API Ìò∏Ï∂ú Ï†úÍ±∞ ‚Üí Rate Limit ÌöåÌîº + ÏÜçÎèÑ Ìñ•ÏÉÅ
    """
    stock_codes = list(candidate_stocks.keys())
    logger.info(f"   (Prefetch) {len(stock_codes)}Í∞ú Ï¢ÖÎ™© Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ï†Ñ Ï°∞Ìöå ÏãúÏûë...")
    
    snapshot_cache: Dict[str, Dict] = {}
    news_cache: Dict[str, str] = {}
    
    prefetch_start = time.time()
    
    # 1. KIS API Ïä§ÎÉÖÏÉ∑ Î≥ëÎ†¨ Ï°∞Ìöå (4Í∞ú ÏõåÏª§)
    logger.info(f"   (Prefetch) KIS Ïä§ÎÉÖÏÉ∑ Ï°∞Ìöå Ï§ë...")
    snapshot_start = time.time()
    
    def fetch_snapshot(code):
        try:
            if hasattr(kis_api, 'API_CALL_DELAY'):
                time.sleep(kis_api.API_CALL_DELAY * 0.3)  # ÏïΩÍ∞ÑÏùò ÎîúÎ†àÏù¥
            return code, kis_api.get_stock_snapshot(code)
        except Exception as e:
            logger.debug(f"   ‚ö†Ô∏è [{code}] Snapshot Ï°∞Ìöå Ïã§Ìå®: {e}")
            return code, None
    
    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = [executor.submit(fetch_snapshot, code) for code in stock_codes]
        for future in as_completed(futures):
            code, snapshot = future.result()
            if snapshot:
                snapshot_cache[code] = snapshot
    
    snapshot_time = time.time() - snapshot_start
    logger.info(f"   (Prefetch) ‚úÖ KIS Ïä§ÎÉÖÏÉ∑ {len(snapshot_cache)}/{len(stock_codes)}Í∞ú Ï°∞Ìöå ÏôÑÎ£å ({snapshot_time:.1f}Ï¥à)")
    
    # 2. ChromaDB Îâ¥Ïä§ Î≥ëÎ†¨ Ï°∞Ìöå (8Í∞ú ÏõåÏª§)
    if vectorstore:
        logger.info(f"   (Prefetch) ChromaDB Îâ¥Ïä§ Ï°∞Ìöå Ï§ë...")
        news_start = time.time()
        
        def fetch_news(code_name):
            code, name = code_name
            try:
                news = fetch_stock_news_from_chroma(vectorstore, code, name, k=3)
                return code, news
            except Exception as e:
                logger.debug(f"   ‚ö†Ô∏è [{code}] Îâ¥Ïä§ Ï°∞Ìöå Ïã§Ìå®: {e}")
                return code, "Îâ¥Ïä§ Ï°∞Ìöå Ïã§Ìå®"
        
        code_name_pairs = [(code, info.get('name', '')) for code, info in candidate_stocks.items()]
        
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = [executor.submit(fetch_news, pair) for pair in code_name_pairs]
            for future in as_completed(futures):
                code, news = future.result()
                news_cache[code] = news
        
        news_time = time.time() - news_start
        valid_news = sum(1 for n in news_cache.values() if n and n not in ["Îâ¥Ïä§ DB ÎØ∏Ïó∞Í≤∞", "ÏµúÍ∑º Í¥ÄÎ†® Îâ¥Ïä§ ÏóÜÏùå", "Îâ¥Ïä§ Í≤ÄÏÉâ Ïò§Î•ò", "Îâ¥Ïä§ Ï°∞Ìöå Ïã§Ìå®"])
        logger.info(f"   (Prefetch) ‚úÖ ChromaDB Îâ¥Ïä§ {valid_news}/{len(stock_codes)}Í∞ú Ï°∞Ìöå ÏôÑÎ£å ({news_time:.1f}Ï¥à)")
    
    total_time = time.time() - prefetch_start
    logger.info(f"   (Prefetch) ‚úÖ Ï†ÑÏ≤¥ ÏÇ¨Ï†Ñ Ï°∞Ìöå ÏôÑÎ£å ({total_time:.1f}Ï¥à)")
    
    return snapshot_cache, news_cache


def enrich_candidates_with_market_data(candidate_stocks: Dict[str, Dict], session, vectorstore) -> None:
    """
    [v4.1] ÌõÑÎ≥¥Íµ∞Ïóê ÏãúÏû• Îç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä (Ìï¥Ïãú Í≥ÑÏÇ∞Ïö©)
    
    Ìï¥ÏãúÏóê Ìè¨Ìï®Îê† Îç∞Ïù¥ÌÑ∞:
    - price: ÏµúÏã† Ï¢ÖÍ∞Ä (5% Î≤ÑÌÇ∑ÌôîÎê®)
    - volume: ÏµúÏã† Í±∞ÎûòÎüâ (10ÎßåÏ£º Î≤ÑÌÇ∑ÌôîÎê®)
    - foreign_net: Ïô∏Íµ≠Ïù∏ ÏàúÎß§Ïàò (Î∞©Ìñ•Îßå - buy/sell/neutral)
    - news_date: ÏµúÏã† Îâ¥Ïä§ ÎÇ†Ïßú (YYYY-MM-DD)
    """
    if not candidate_stocks:
        return
    
    stock_codes = list(candidate_stocks.keys())
    logger.info(f"   (Hash) {len(stock_codes)}Í∞ú Ï¢ÖÎ™© ÏãúÏû• Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå Ï§ë...")
    
    # 1. DBÏóêÏÑú ÏµúÏã† Í∞ÄÍ≤©/Í±∞ÎûòÎüâ Îç∞Ïù¥ÌÑ∞ ÏùºÍ¥Ñ Ï°∞Ìöå
    try:
        from sqlalchemy import text
        
        placeholders = ','.join([f"'{code}'" for code in stock_codes])
        
        # ÏµúÏã† ÎÇ†ÏßúÏùò Îç∞Ïù¥ÌÑ∞Îßå Ï°∞Ìöå (Í∞ÄÍ≤©, Í±∞ÎûòÎüâ)
        query = text(f"""
            SELECT STOCK_CODE, CLOSE_PRICE, VOLUME, PRICE_DATE
            FROM STOCK_DAILY_PRICES_3Y
            WHERE STOCK_CODE IN ({placeholders})
            AND (STOCK_CODE, PRICE_DATE) IN (
                SELECT STOCK_CODE, MAX(PRICE_DATE) 
                FROM STOCK_DAILY_PRICES_3Y
                WHERE STOCK_CODE IN ({placeholders})
                GROUP BY STOCK_CODE
            )
        """)
        rows = session.execute(query).fetchall()
        
        for row in rows:
            code = row[0]
            price = row[1]
            volume = row[2]
            
            if code in candidate_stocks:
                candidate_stocks[code]['price'] = float(price) if price else 0
                candidate_stocks[code]['volume'] = int(volume) if volume else 0
        
        logger.info(f"   (Hash) ‚úÖ DBÏóêÏÑú {len(rows)}Í∞ú Ï¢ÖÎ™© ÏãúÏû• Îç∞Ïù¥ÌÑ∞ Î°úÎìú")
    except Exception as e:
        logger.warning(f"   (Hash) ‚ö†Ô∏è DB ÏãúÏû• Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå Ïã§Ìå®: {e}")
    
    # 2. ChromaDB Îâ¥Ïä§ Ï°∞Ìöå ÏÉùÎûµ (ÏÜçÎèÑ ÏµúÏ†ÅÌôî)
    # Ïù¥Ïú†: Ìï¥ÏãúÏóê Ïò§Îäò ÎÇ†ÏßúÍ∞Ä Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏñ¥ÏÑú Îß§Ïùº Ïû¨ÌèâÍ∞Ä Î≥¥Ïû•Îê®
    # Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞Îäî Phase 1 HunterÏóêÏÑú Í∞úÎ≥Ñ Ï¢ÖÎ™© ÌèâÍ∞Ä Ïãú Ï°∞ÌöåÌï®
    logger.info(f"   (Hash) ‚úÖ Îâ¥Ïä§ ÎÇ†Ïßú Ï°∞Ìöå ÏÉùÎûµ (ÎÇ†Ïßú Í∏∞Î∞ò Ï∫êÏãú Î¨¥Ìö®ÌôîÎ°ú ÎåÄÏ≤¥)")


def _get_latest_news_date(vectorstore, stock_code: str, stock_name: str) -> Optional[str]:
    """ChromaDBÏóêÏÑú Ï¢ÖÎ™©Ïùò ÏµúÏã† Îâ¥Ïä§ ÎÇ†Ïßú Ï°∞Ìöå"""
    try:
        docs = vectorstore.similarity_search(
            query=f"{stock_name}",
            k=1,
            filter={"stock_code": stock_code}
        )
        if docs and docs[0].metadata:
            # Îâ¥Ïä§ ÎÇ†ÏßúÎ•º YYYY-MM-DD ÌòïÏãùÏúºÎ°ú Î∞òÌôò
            news_date = docs[0].metadata.get('date') or docs[0].metadata.get('published_at')
            if news_date:
                # ÎÇ†Ïßú Î¨∏ÏûêÏó¥ÏóêÏÑú YYYY-MM-DDÎßå Ï∂îÏ∂ú
                return str(news_date)[:10]
    except Exception:
        pass
    return None


def _record_to_cache_payload(record: Dict) -> Dict:
    metadata = record.get("llm_metadata", {})
    return {
        "code": record["code"],
        "name": record["name"],
        "llm_score": record.get("llm_score", 0),
        "llm_reason": record.get("llm_reason", ""),
        "llm_grade": metadata.get("llm_grade"),
        "decision_hash": metadata.get("decision_hash"),
        "llm_updated_at": metadata.get("llm_updated_at"),
        "is_tradable": record.get("is_tradable", True),
        "approved": record.get("approved", False),
    }


def _cache_payload_to_record(entry: Dict, decision_hash: str) -> Dict:
    updated_at = entry.get("llm_updated_at")
    metadata = {
        "llm_grade": entry.get("llm_grade"),
        "decision_hash": decision_hash,
        "llm_updated_at": updated_at,
        "source": "cache",
    }
    return {
        "code": entry["code"],
        "name": entry.get("name", entry["code"]),
        "llm_score": entry.get("llm_score", 0),
        "llm_reason": entry.get("llm_reason", ""),
        "is_tradable": entry.get("is_tradable", True),
        "approved": entry.get("approved", False),
        "llm_metadata": metadata,
    }

# ÏÑπÌÑ∞/ÌÖåÎßà Î∂ÑÏÑù Ìï®ÏàòÎì§ÏùÄ scout_universe.pyÏóêÏÑú importÎê®
# (analyze_sector_momentum, get_hot_sector_stocks, get_dynamic_blue_chips, get_momentum_stocks)

# ÏûêÎèô ÌååÎùºÎØ∏ÌÑ∞ ÏµúÏ†ÅÌôî Ìï®ÏàòÎì§ÏùÄ scout_optimizer.pyÏóêÏÑú importÎê®
# (run_auto_parameter_optimization, run_simple_backtest, generate_optimized_params, verify_params_with_llm)


def fetch_stock_news_from_chroma(vectorstore, stock_code: str, stock_name: str, k: int = 3) -> str:
    """
    [v3.9] ChromaDBÏóêÏÑú Ï¢ÖÎ™©Î≥Ñ ÏµúÏã† Îâ¥Ïä§ Í≤ÄÏÉâ
    
    Args:
        vectorstore: ChromaDB vectorstore Ïù∏Ïä§ÌÑ¥Ïä§
        stock_code: Ï¢ÖÎ™© ÏΩîÎìú
        stock_name: Ï¢ÖÎ™©Î™Ö
        k: Í∞ÄÏ†∏Ïò¨ Îâ¥Ïä§ Í∞úÏàò
        
    Returns:
        Îâ¥Ïä§ ÏöîÏïΩ Î¨∏ÏûêÏó¥ (ÏóÜÏúºÎ©¥ "ÏµúÍ∑º Í¥ÄÎ†® Îâ¥Ïä§ ÏóÜÏùå")
    """
    if not vectorstore:
        return "Îâ¥Ïä§ DB ÎØ∏Ïó∞Í≤∞"
    
    try:
        from datetime import datetime, timedelta, timezone
        
        # ÏµúÏã† 7Ïùº Ïù¥ÎÇ¥ Îâ¥Ïä§ ÌïÑÌÑ∞
        recency_timestamp = int((datetime.now(timezone.utc) - timedelta(days=7)).timestamp())
        
        # Ï¢ÖÎ™© ÏΩîÎìúÎ°ú ÌïÑÌÑ∞ÎßÅÎêú Îâ¥Ïä§ Í≤ÄÏÉâ ÏãúÎèÑ
        try:
            docs = vectorstore.similarity_search(
                query=f"{stock_name} Ïã§Ï†Å ÏàòÏ£º Ìò∏Ïû¨",
                k=k,
                filter={"stock_code": stock_code}
            )
            # logger.debug(f"   (D) [{stock_code}] ÌïÑÌÑ∞ Í≤ÄÏÉâ Í≤∞Í≥º: {len(docs)}Í±¥")
        except Exception:
            # ÌïÑÌÑ∞ Ïã§Ìå®Ïãú Ï¢ÖÎ™©Î™ÖÏúºÎ°ú Í≤ÄÏÉâ
            docs = vectorstore.similarity_search(
                query=f"{stock_name} Ï£ºÏãù Îâ¥Ïä§",
                k=k
            )
            logger.debug(f"   (D) [{stock_code}] Ï¢ÖÎ™©Î™Ö Í≤ÄÏÉâ(Fallback): {len(docs)}Í±¥")
            # Ï¢ÖÎ™© Í¥ÄÎ†® Îâ¥Ïä§Îßå ÌïÑÌÑ∞ÎßÅ
            docs = [d for d in docs if stock_name in d.page_content or stock_code in str(d.metadata)]
        
        if docs:
            news_items = []
            for i, doc in enumerate(docs[:k], 1):
                content = doc.page_content[:100].strip()
                if content:
                    news_items.append(f"[Îâ¥Ïä§{i}] {content}")
            
            if news_items:
                return " | ".join(news_items)
        
        return "ÏµúÍ∑º Í¥ÄÎ†® Îâ¥Ïä§ ÏóÜÏùå"
        
    except Exception as e:
        logger.debug(f"   ‚ö†Ô∏è [{stock_code}] ChromaDB Îâ¥Ïä§ Í≤ÄÏÉâ Ïò§Î•ò: {e}")
        return "Îâ¥Ïä§ Í≤ÄÏÉâ Ïò§Î•ò"


# =============================================================================
# [v1.0 Refactored] ÌååÏù¥ÌîÑÎùºÏù∏ ÌÉúÏä§ÌÅ¨ Ìï®ÏàòÎì§ÏùÄ scout_pipeline.pyÎ°ú Î∂ÑÎ¶¨Îê®
# - is_hybrid_scoring_enabled, process_quant_scoring_task
# - process_phase1_hunter_v5_task, process_phase23_judge_v5_task
# - process_phase1_hunter_task, process_phase23_debate_judge_task
# - process_llm_decision_task, fetch_kis_data_task
# =============================================================================

def main():
    start_time = time.time()
    logger.info("--- ü§ñ 'Scout Job' [v3.0 Local] Ïã§Ìñâ ÏãúÏûë ---")
    
    kis_api = None
    brain = None

    try:
        logger.info("--- [Init] ÌôòÍ≤Ω Î≥ÄÏàò Î°úÎìú Î∞è KIS API Ïó∞Í≤∞ ÏãúÏûë ---")
        load_dotenv(override=True)
        
        trading_mode = os.getenv("TRADING_MODE", "REAL")
        use_gateway = os.getenv("USE_KIS_GATEWAY", "true").lower() == "true"
        
        if use_gateway:
            kis_api = KISGatewayClient()
            logger.info("‚úÖ KIS Gateway Client Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        else:
            kis_api = KIS_API(
                app_key=auth.get_secret(os.getenv(f"{trading_mode}_SECRET_ID_APP_KEY")),
                app_secret=auth.get_secret(os.getenv(f"{trading_mode}_SECRET_ID_APP_SECRET")),
                base_url=os.getenv(f"KIS_BASE_URL_{trading_mode}"),
                account_prefix=auth.get_secret(os.getenv(f"{trading_mode}_SECRET_ID_ACCOUNT_PREFIX")),
                account_suffix=os.getenv("KIS_ACCOUNT_SUFFIX"),
                token_file_path="/app/tokens/kis_token_scout.json",
                trading_mode=trading_mode
            )
            if not kis_api.authenticate():
                raise Exception("KIS API Ïù∏Ï¶ùÏóê Ïã§Ìå®ÌñàÏäµÎãàÎã§.")
        
        brain = JennieBrain(
            project_id=os.getenv("GCP_PROJECT_ID", "local"),
            gemini_api_key_secret=os.getenv("SECRET_ID_GEMINI_API_KEY")
        )
        
        # [v4.3] SQLAlchemy ÏÑ∏ÏÖò Ï¥àÍ∏∞Ìôî (session_scope ÏÇ¨Ïö© Ï†ÑÏóê Ìò∏Ï∂ú ÌïÑÏàò)
        ensure_engine_initialized()
        
        # [v4.3] SQLAlchemy ÏÑ∏ÏÖò ÏÇ¨Ïö©ÏúºÎ°ú Î≥ÄÍ≤Ω
        with session_scope() as session:
            watchlist_snapshot = database.get_active_watchlist(session)
            
            vectorstore = None
            try:
                logger.info("   ... ChromaDB ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ïó∞Í≤∞ ÏãúÎèÑ (Gemini Embeddings) ...")
                api_key = ensure_gemini_api_key()
                embeddings = GoogleGenerativeAIEmbeddings(
                    model="models/gemini-embedding-001", 
                    google_api_key=api_key
                )
                
                chroma_client = chromadb.HttpClient( # noqa
                    host=CHROMA_SERVER_HOST, 
                    port=CHROMA_SERVER_PORT
                )
                vectorstore = Chroma(
                    client=chroma_client, 
                    collection_name="rag_stock_data", 
                    embedding_function=embeddings
                )
                logger.info("‚úÖ [v3.0] LLM Î∞è ChromaDB ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å.")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è ChromaDB Ï¥àÍ∏∞Ìôî Ïã§Ìå® (RAG Í∏∞Îä• ÎπÑÌôúÏÑ±Ìôî): {e}")
                vectorstore = None

            # Phase 1: Ìä∏Î¶¨Ìîå ÏÜåÏä§ ÌõÑÎ≥¥ Î∞úÍµ¥ (v3.8: ÏÑπÌÑ∞ Î∂ÑÏÑù Ï∂îÍ∞Ä)
            logger.info("--- [Phase 1] Ìä∏Î¶¨Ìîå ÏÜåÏä§ ÌõÑÎ≥¥ Î∞úÍµ¥ ÏãúÏûë ---")
            update_pipeline_status(phase=1, phase_name="Hunter Scout", status="running", progress=0)
            candidate_stocks = {}

            # A: ÎèôÏ†Å Ïö∞ÎüâÏ£º (KOSPI 200 Í∏∞Ï§Ä)
            universe_size = int(os.getenv("SCOUT_UNIVERSE_SIZE", "200"))
            for stock in get_dynamic_blue_chips(limit=universe_size):
                candidate_stocks[stock['code']] = {'name': stock['name'], 'reasons': ['KOSPI ÏãúÏ¥ù ÏÉÅÏúÑ']}
            
            # E: ÏÑπÌÑ∞ Î™®Î©òÌÖÄ Î∂ÑÏÑù (v3.8 Ïã†Í∑ú)
            sector_analysis = analyze_sector_momentum(kis_api, session, watchlist_snapshot)
            hot_sector_stocks = get_hot_sector_stocks(sector_analysis, top_n=30)
            for stock in hot_sector_stocks:
                if stock['code'] not in candidate_stocks:
                    candidate_stocks[stock['code']] = {
                        'name': stock['name'], 
                        'reasons': [f"Ìï´ ÏÑπÌÑ∞ ({stock['sector']}, +{stock['sector_momentum']:.1f}%)"]
                    }
                else:
                    candidate_stocks[stock['code']]['reasons'].append(
                        f"Ìï´ ÏÑπÌÑ∞ ({stock['sector']}, +{stock['sector_momentum']:.1f}%)"
                    )

            # B: Ï†ïÏ†Å Ïö∞ÎüâÏ£º
            for stock in BLUE_CHIP_STOCKS:
                if stock['code'] not in candidate_stocks:
                    candidate_stocks[stock['code']] = {'name': stock['name'], 'reasons': ['Ï†ïÏ†Å Ïö∞ÎüâÏ£º']}

            # C: RAG
            if vectorstore:
                try:
                    logger.info("   (C) RAG Í∏∞Î∞ò ÌõÑÎ≥¥ Î∞úÍµ¥ Ï§ë...")
                    rag_results = vectorstore.similarity_search(query="Ïã§Ï†Å Ìò∏Ïû¨ Í≥ÑÏïΩ ÏàòÏ£º", k=50)
                    for doc in rag_results:
                        stock_code = doc.metadata.get('stock_code')
                        stock_name = doc.metadata.get('stock_name')
                        if stock_code and stock_name:
                            if stock_code not in candidate_stocks:
                                candidate_stocks[stock_code] = {'name': stock_name, 'reasons': []}
                            candidate_stocks[stock_code]['reasons'].append(f"RAG Ìè¨Ï∞©: {doc.page_content[:20]}...")
                except Exception as e:
                    logger.warning(f"   (C) RAG Í≤ÄÏÉâ Ïã§Ìå®: {e}")

            # D: Î™®Î©òÌÖÄ
            logger.info("   (D) Î™®Î©òÌÖÄ Ìå©ÌÑ∞ Í∏∞Î∞ò Ï¢ÖÎ™© Î∞úÍµ¥ Ï§ë...")
            momentum_stocks = get_momentum_stocks(
                    kis_api,
                    session,
                period_months=6,
                top_n=30,
                watchlist_snapshot=watchlist_snapshot
            )
            for stock in momentum_stocks:
                if stock['code'] not in candidate_stocks:
                    candidate_stocks[stock['code']] = {
                        'name': stock['name'], 
                        'reasons': [f'Î™®Î©òÌÖÄ ({stock["momentum"]:.1f}%)']
                    }
            
            logger.info(f"   ‚úÖ ÌõÑÎ≥¥Íµ∞ {len(candidate_stocks)}Í∞ú Î∞úÍµ¥ ÏôÑÎ£å.")

            # [v4.1] Ìï¥Ïãú Í≥ÑÏÇ∞ Ï†ÑÏóê ÏãúÏû• Îç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä (Í∞ÄÍ≤©, Í±∞ÎûòÎüâ)
            logger.info("--- [Phase 1.5] ÏãúÏû• Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò Ìï¥Ïãú Í≥ÑÏÇ∞ ---")
            enrich_candidates_with_market_data(candidate_stocks, session, vectorstore)
            
            # [v4.2] Phase 1 ÏãúÏûë Ï†ÑÏóê Î™®Îì† Îç∞Ïù¥ÌÑ∞ ÏùºÍ¥Ñ Ï°∞Ìöå (Î≥ëÎ†¨ Ïä§Î†àÎìú Ïïà API Ìò∏Ï∂ú Ï†úÍ±∞)
            logger.info("--- [Phase 1.6] Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ï†Ñ Ï°∞Ìöå (Ïä§ÎÉÖÏÉ∑/Îâ¥Ïä§) ---")
            snapshot_cache, news_cache = prefetch_all_data(candidate_stocks, kis_api, vectorstore)

            # [v4.3] Îâ¥Ïä§ Ìï¥ÏãúÎ•º candidate_stocksÏóê Î∞òÏòÅ (Ìï¥Ïãú Í≥ÑÏÇ∞Ïóê Ìè¨Ìï®)
            # Îâ¥Ïä§ ÎÇ¥Ïö©Ïù¥ Î∞îÎÄåÎ©¥ Ìï¥ÏãúÍ∞Ä Îã¨ÎùºÏ†∏ LLM Ïû¨Ìò∏Ï∂úÎê®
            news_hash_count = 0
            for code, news in news_cache.items():
                if code in candidate_stocks and news and news not in [
                    "Îâ¥Ïä§ DB ÎØ∏Ïó∞Í≤∞", "ÏµúÍ∑º Í¥ÄÎ†® Îâ¥Ïä§ ÏóÜÏùå", "Îâ¥Ïä§ Í≤ÄÏÉâ Ïò§Î•ò", 
                    "Îâ¥Ïä§ Ï°∞Ìöå Ïã§Ìå®", "Îâ¥Ïä§ Ï∫êÏãú ÏóÜÏùå"
                ]:
                    # Îâ¥Ïä§ ÎÇ¥Ïö©Ïùò MD5 Ìï¥Ïãú (ÏãúÍ∞Ñ Ï†ïÎ≥¥ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏùå)
                    candidate_stocks[code]['news_hash'] = hashlib.md5(news.encode()).hexdigest()[:16]
                    news_hash_count += 1
            logger.info(f"   (Hash) ‚úÖ Îâ¥Ïä§ Ìï¥Ïãú {news_hash_count}Í∞ú Î∞òÏòÅ ÏôÑÎ£å")

            # [v4.0] Phase 1.8: ÏàòÍ∏â Îç∞Ïù¥ÌÑ∞(Market Flow) Î∂ÑÏÑù Î∞è Í∏∞Î°ù
            logger.info("--- [Phase 1.8] ÏàòÍ∏â Îç∞Ïù¥ÌÑ∞(Market Flow) Î∂ÑÏÑù (Foreign/Institution) ---")
            
            # [Optimization] Î≥ëÎ†¨Î°ú Ìà¨ÏûêÏûê ÎèôÌñ• Ï°∞Ìöå
            investor_flow_cache = {}
            
            # Archivist Ï¥àÍ∏∞Ìôî (Ïó¨Í∏∞ÏÑúÎèÑ ÏÇ¨Ïö©)
            if 'archivist' not in locals():
                archivist = Archivist(session_scope)
                
            def process_flow_data(code):
                try:
                    # ÏµúÍ∑º 1ÏùºÏπò(Ïò§Îäò/Ïñ¥Ï†ú) Îç∞Ïù¥ÌÑ∞Îßå Ï°∞ÌöåÌïòÏó¨ ÌòÑÏû¨ ÏàòÍ∏â ÌôïÏù∏
                    # Ïû• Ï§ëÏù¥Î©¥ Ïò§Îäò Ïû†Ï†ïÏπò/ÌôïÏ†ïÏπò, Ïû• ÎßàÍ∞ê ÌõÑÎ©¥ Ïò§Îäò ÌôïÏ†ïÏπò
                    trends = kis_api.get_market_data().get_investor_trend(code, start_date=None, end_date=None)
                    if not trends:
                        return code, None
                    
                    # Í∞ÄÏû• ÏµúÍ∑º Îç∞Ïù¥ÌÑ∞ (Ïò§Îäò)
                    latest = trends[-1]
                    return code, latest
                except Exception as e:
                    return code, None

            with ThreadPoolExecutor(max_workers=8) as executor:
                futures = [executor.submit(process_flow_data, code) for code in candidate_stocks.keys()]
                for future in as_completed(futures):
                    code, flow_data = future.result()
                    if flow_data:
                        investor_flow_cache[code] = flow_data
                        
                        # ÌõÑÎ≥¥Íµ∞ Ï†ïÎ≥¥Ïóê ÏàòÍ∏â Îç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä (LLM ÌîÑÎ°¨ÌîÑÌä∏Ïö©)
                        candidate_stocks[code]['market_flow'] = {
                            'foreign_net_buy': flow_data['foreigner_net_buy'],
                            'institution_net_buy': flow_data['institution_net_buy'],
                            'individual_net_buy': flow_data['individual_net_buy']
                        }
                        
                        # ArchivistÏóê Í∏∞Î°ù (Market Flow Snapshot)
                        try:
                            # flow_dataÎäî dict ÌòïÌÉú (date, price, foreign..., institution...)
                            # Archivist.log_market_flow_snapshotÏùÄ stock_codeÎ•º Ìè¨Ìï®Ìïú dictÎ•º Í∏∞ÎåÄÌï®
                            log_payload = flow_data.copy()
                            log_payload['stock_code'] = code
                            # volume ÌïÑÎìúÍ∞Ä get_investor_trend Í≤∞Í≥ºÏóê ÏóÜÏúºÎØÄÎ°ú (ÌïÑÏöîÏãú) Î≥¥ÏôÑ
                            # log_payload['volume'] = ... 
                            
                            archivist.log_market_flow_snapshot(log_payload)
                        except Exception as log_e:
                            logger.warning(f"Failed to log market flow for {code}: {log_e}")

            logger.info(f"   (Flow) ‚úÖ ÏàòÍ∏â Îç∞Ïù¥ÌÑ∞ {len(investor_flow_cache)}Í∞ú Ï¢ÖÎ™© Î∂ÑÏÑù Î∞è Í∏∞Î°ù ÏôÑÎ£å")

            # Phase 2: LLM ÏµúÏ¢Ö ÏÑ†Ï†ï
            logger.info("--- [Phase 2] LLM Í∏∞Î∞ò ÏµúÏ¢Ö Watchlist ÏÑ†Ï†ï ÏãúÏûë ---")
            update_pipeline_status(
                phase=1, phase_name="Hunter Scout", status="running", 
                total_candidates=len(candidate_stocks)
            )
            
            # =============================================================
            # [v1.0] ÌïòÏù¥Î∏åÎ¶¨Îìú Ïä§ÏΩîÏñ¥ÎßÅ Î™®Îìú Î∂ÑÍ∏∞
            # =============================================================
            if is_hybrid_scoring_enabled():
                logger.info("=" * 60)
                logger.info("   üöÄ Scout v5 Hybrid Scoring Mode ÌôúÏÑ±Ìôî!")
                logger.info("=" * 60)
                
                try:
                    from shared.hybrid_scoring import (
                        QuantScorer, HybridScorer, 
                        create_hybrid_scoring_tables,
                        format_quant_score_for_prompt,
                    )
                    from shared.market_regime import MarketRegimeDetector
                    
                    # DB ÌÖåÏù¥Î∏î ÏÉùÏÑ± ÌôïÏù∏
                    create_hybrid_scoring_tables(session)
                    
                    # ÏãúÏû• Íµ≠Î©¥ Í∞êÏßÄ
                    kospi_prices = database.get_daily_prices(session, "0001", limit=60)
                    if not kospi_prices.empty:
                        detector = MarketRegimeDetector()
                        current_regime, _ = detector.detect_regime(kospi_prices, float(kospi_prices['CLOSE_PRICE'].iloc[-1]), quiet=True)
                    else:
                        current_regime = "SIDEWAYS"
                    
                    logger.info(f"   ÌòÑÏû¨ ÏãúÏû• Íµ≠Î©¥: {current_regime}")
                    
                    # QuantScorer Ï¥àÍ∏∞Ìôî
                    quant_scorer = QuantScorer(session, market_regime=current_regime)
                    
                    # Step 1: Ï†ïÎüâ Ï†êÏàò Í≥ÑÏÇ∞ (LLM Ìò∏Ï∂ú ÏóÜÏùå, ÎπÑÏö© 0Ïõê)
                    logger.info(f"\n   [v5 Step 1] Ï†ïÎüâ Ï†êÏàò Í≥ÑÏÇ∞ ({len(candidate_stocks)}Í∞ú Ï¢ÖÎ™©) - ÎπÑÏö© 0Ïõê")
                    quant_results = {}
                    
                    for code, info in candidate_stocks.items():
                        if code == '0001':
                            continue
                        stock_info = {
                            'code': code,
                            'info': info,
                            'snapshot': snapshot_cache.get(code),
                        }
                        quant_results[code] = process_quant_scoring_task(
                            stock_info, quant_scorer, session, kospi_prices
                        )
                    
                    # Step 2: Ï†ïÎüâ Í∏∞Î∞ò 1Ï∞® ÌïÑÌÑ∞ÎßÅ (ÌïòÏúÑ 20% ÌÉàÎùΩ) - [v1.1] ÌïÑÌÑ∞ÎßÅ ÏôÑÌôî
                    logger.info(f"\n   [v5 Step 2] Ï†ïÎüâ Í∏∞Î∞ò 1Ï∞® ÌïÑÌÑ∞ÎßÅ (ÌïòÏúÑ 20% ÌÉàÎùΩ)")
                    quant_result_list = list(quant_results.values())
                    filtered_results = quant_scorer.filter_candidates(quant_result_list, cutoff_ratio=0.2)
                    
                    filtered_codes = {r.stock_code for r in filtered_results}
                    logger.info(f"   ‚úÖ Ï†ïÎüâ ÌïÑÌÑ∞ ÌÜµÍ≥º: {len(filtered_codes)}Í∞ú (ÌèâÍ∑† Ï†êÏàò: {sum(r.total_score for r in filtered_results)/len(filtered_results):.1f}Ï†ê)")
                    
                    # Step 3: LLM Ï†ïÏÑ± Î∂ÑÏÑù (ÌÜµÍ≥º Ï¢ÖÎ™©Îßå)
                    logger.info(f"\n   [v5 Step 3] LLM Ï†ïÏÑ± Î∂ÑÏÑù (ÌÜµÍ≥Ñ Ïª®ÌÖçÏä§Ìä∏ Ìè¨Ìï®)")
                    
                    final_approved_list: List[Dict] = []
                    if '0001' in candidate_stocks:
                        final_approved_list.append({'code': '0001', 'name': 'KOSPI', 'is_tradable': False})
                    
                    llm_decision_records: Dict[str, Dict] = {}
                    llm_max_workers = max(1, _parse_int_env(os.getenv("SCOUT_LLM_MAX_WORKERS"), 4))
                    
                    # Phase 1: Hunter (ÌÜµÍ≥Ñ Ïª®ÌÖçÏä§Ìä∏ Ìè¨Ìï®)
                    phase1_results = []
                    # [v6.0] Archivist Ï¥àÍ∏∞Ìôî (Phase 1/2 Í≥µÏö©)
                    archivist = Archivist(session_scope)

                    with ThreadPoolExecutor(max_workers=llm_max_workers) as executor:
                        future_to_code = {}
                        for code in filtered_codes:
                            info = candidate_stocks[code]
                            quant_result = quant_results[code]
                            payload = {'code': code, 'info': info}
                            future = executor.submit(
                                process_phase1_hunter_v5_task, 
                                payload, brain, quant_result, snapshot_cache, news_cache, archivist
                            )
                            future_to_code[future] = code
                        
                        for future in as_completed(future_to_code):
                            result = future.result()
                            if result:
                                phase1_results.append(result)
                                if not result['passed']:
                                    llm_decision_records[result['code']] = {
                                        'code': result['code'],
                                        'name': result['name'],
                                        'llm_score': result['hunter_score'],
                                        'llm_reason': result['hunter_reason'],
                                        'is_tradable': False,
                                        'approved': False,
                                        'hunter_score': result['hunter_score'],
                                        'llm_metadata': {'llm_grade': 'D', 'source': 'v5_hunter_reject'}
                                    }
                    
                    phase1_passed = [r for r in phase1_results if r['passed']]
                    logger.info(f"   ‚úÖ v5 Hunter ÌÜµÍ≥º: {len(phase1_passed)}/{len(filtered_codes)}Í∞ú")
                    
                    # Phase 2-3: Debate + Judge (ÏÉÅÏúÑ Ï¢ÖÎ™©Îßå)
                    PHASE2_MAX = int(os.getenv("SCOUT_PHASE2_MAX_ENTRIES", "50"))
                    if len(phase1_passed) > PHASE2_MAX:
                        phase1_passed_sorted = sorted(phase1_passed, key=lambda x: x['hunter_score'], reverse=True)
                        phase1_passed = phase1_passed_sorted[:PHASE2_MAX]
                    
                    if phase1_passed:
                        logger.info(f"\n   [v5 Step 4] Debate + Judge (ÌïòÏù¥Î∏åÎ¶¨Îìú Ï†êÏàò Í≤∞Ìï©)")
                        
                        with ThreadPoolExecutor(max_workers=llm_max_workers) as executor:
                            future_to_code = {}
                            
                            # [v6.0] Archivist ÏÇ¨Ïö© (ÏúÑÏóêÏÑú Ï¥àÍ∏∞ÌôîÎê®)

                            for p1_result in phase1_passed:
                                future = executor.submit(
                                    process_phase23_judge_v5_task, 
                                    p1_result, brain, archivist, current_regime
                                )
                                future_to_code[future] = p1_result['code']
                            
                            for future in as_completed(future_to_code):
                                record = future.result()
                                if record:
                                    llm_decision_records[record['code']] = record
                                    if record.get('approved'):
                                        final_approved_list.append(_record_to_watchlist_entry(record))
                    
                    logger.info(f"   ‚úÖ v5 ÏµúÏ¢Ö ÏäπÏù∏: {len([r for r in llm_decision_records.values() if r.get('approved')])}Í∞ú")
                    
                    # ÏøºÌÑ∞Ï†ú Ï†ÅÏö©
                    MAX_WATCHLIST_SIZE = 15
                    if len(final_approved_list) > MAX_WATCHLIST_SIZE:
                        final_approved_list_sorted = sorted(
                            final_approved_list,
                            key=lambda x: x.get('llm_score', 0),
                            reverse=True
                        )
                        final_approved_list = final_approved_list_sorted[:MAX_WATCHLIST_SIZE]
                    
                    logger.info(f"\n   üèÅ Scout v1.0 ÏôÑÎ£å: {len(final_approved_list)}Í∞ú Ï¢ÖÎ™© ÏÑ†Ï†ï")
                    _v5_completed = True
                    
                except Exception as e:
                    logger.error(f"‚ùå Scout v1.0 Ïã§Ìñâ Ïò§Î•ò, v4 Î™®ÎìúÎ°ú Ìè¥Î∞±: {e}", exc_info=True)
                    _v5_completed = False
            else:
                _v5_completed = False
            
            # =============================================================
            # [v4.x] Í∏∞Ï°¥ LLM Í∏∞Î∞ò ÏÑ†Ï†ï Î°úÏßÅ (v5 ÎØ∏ÌôúÏÑ±Ìôî ÎòêÎäî Ïã§Ìå® Ïãú)
            # =============================================================
            if not _v5_completed:
                logger.info("   (Mode) v4.x Í∏∞Ï°¥ LLM Í∏∞Î∞ò Î°úÏßÅ Ïã§Ìñâ")
                
                # [v4.3] ÏÉàÎ°úÏö¥ Ï∫êÏãú ÏãúÏä§ÌÖú - LLM_EVAL_CACHE ÌÖåÏù¥Î∏î Í∏∞Î∞ò ÏßÅÏ†ë ÎπÑÍµê (db_conn ÏÇ¨Ïö©)
                llm_cache_snapshot = _load_llm_cache_from_db(session)
                llm_max_workers = max(1, _parse_int_env(os.getenv("SCOUT_LLM_MAX_WORKERS"), 4))
    
                # Ïò§Îäò ÎÇ†Ïßú (KST Í∏∞Ï§Ä)
                kst = timezone(timedelta(hours=9))
                today_str = datetime.now(kst).strftime("%Y-%m-%d")
    
                final_approved_list: List[Dict] = []
                if '0001' in candidate_stocks:
                    final_approved_list.append({'code': '0001', 'name': 'KOSPI', 'is_tradable': False})
                    del candidate_stocks['0001']
    
                llm_decision_records: Dict[str, Dict] = {}
                cache_hits = 0
                pending_codes: List[str] = []
                cache_miss_reasons: Dict[str, str] = {}  # ÎîîÎ≤ÑÍπÖÏö©
    
                for code, info in candidate_stocks.items():
                    cached = llm_cache_snapshot.get(code)
                    
                    # [v4.3] ÏßÅÏ†ë ÎπÑÍµêÎ°ú Ï∫êÏãú Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù
                    current_data = {
                        'price_bucket': _get_price_bucket(info.get('price', 0)),
                        'volume_bucket': _get_volume_bucket(info.get('volume', 0)),
                        'news_hash': info.get('news_hash'),
                    }
                    
                    if _is_cache_valid_direct(cached, current_data, today_str):
                        # Ï∫êÏãú Ï†ÅÏ§ë - Ïù¥Ï†Ñ LLM Í≤∞Í≥º Ïû¨ÏÇ¨Ïö©
                        llm_decision_records[code] = {
                            'code': code,
                            'name': info['name'],
                            'llm_score': cached.get('judge_score') or cached.get('hunter_score', 0),
                            'llm_reason': cached.get('llm_reason', ''),
                            'is_tradable': cached.get('is_tradable', False),
                            'approved': cached.get('is_approved', False),
                            'llm_metadata': {
                                'llm_grade': cached.get('llm_grade'),
                                'source': 'cache',
                            }
                        }
                        cache_hits += 1
                        if cached.get('is_approved'):
                            final_approved_list.append(_record_to_watchlist_entry(llm_decision_records[code]))
                    else:
                        # Ï∫êÏãú ÎØ∏Ïä§ - LLM Ïû¨Ìò∏Ï∂ú ÌïÑÏöî
                        pending_codes.append(code)
                        # ÎØ∏Ïä§ ÏõêÏù∏ Í∏∞Î°ù (ÎîîÎ≤ÑÍπÖÏö©)
                        if not cached:
                            cache_miss_reasons[code] = "no_cache"
                        elif cached.get('eval_date') != today_str:
                            cache_miss_reasons[code] = f"date({cached.get('eval_date')}!={today_str})"
                        elif cached.get('price_bucket') != current_data['price_bucket']:
                            cache_miss_reasons[code] = f"price({cached.get('price_bucket')}!={current_data['price_bucket']})"
                        elif (cached.get('news_hash') or '') != (current_data.get('news_hash') or ''):
                            cache_miss_reasons[code] = "news_changed"
    
                if cache_hits:
                    logger.info(f"   (LLM) ‚úÖ Ï∫êÏãú Ï†ÅÏ§ë {cache_hits}Í±¥ (Ïò§Îäò ÎÇ†Ïßú + ÎèôÏùº Í∞ÄÍ≤©/Îâ¥Ïä§)")
                
                if pending_codes:
                    # Ï∫êÏãú ÎØ∏Ïä§ ÏõêÏù∏ Î∂ÑÏÑù
                    reason_counts = {}
                    for reason in cache_miss_reasons.values():
                        reason_type = reason.split("(")[0]
                        reason_counts[reason_type] = reason_counts.get(reason_type, 0) + 1
                    logger.info(f"   (LLM) ‚ö†Ô∏è Ï∫êÏãú ÎØ∏Ïä§ {len(pending_codes)}Í±¥ - ÏõêÏù∏: {reason_counts}")
    
                need_llm_calls = len(pending_codes) > 0
    
                llm_invocation_count = 0
                if need_llm_calls:
                    if brain is None:
                        logger.error("   (LLM) JennieBrain Ï¥àÍ∏∞Ìôî Ïã§Ìå®Î°ú Ïã†Í∑ú Ìò∏Ï∂úÏùÑ ÏàòÌñâÌï† Ïàò ÏóÜÏäµÎãàÎã§.")
                    else:
                        # [v3.8] 2-Pass Î≥ëÎ†¨ Ï≤òÎ¶¨ ÏµúÏ†ÅÌôî
                        # Pass 1: Phase 1 Hunter (Gemini-Flash) - Î≥ëÎ†¨Î°ú Îπ†Î•¥Í≤å ÌïÑÌÑ∞ÎßÅ
                        logger.info(f"   (LLM) [Pass 1] Phase 1 Hunter Î≥ëÎ†¨ Ïã§Ìñâ ÏãúÏûë ({len(pending_codes)}Í∞ú Ï¢ÖÎ™©)")
                        update_pipeline_status(
                            phase=1, phase_name="Hunter Scout", status="running",
                            total_candidates=len(candidate_stocks)
                        )
                        phase1_start = time.time()
                        
                        phase1_results = []
                        # [v4.1] Claude Rate Limit ÎåÄÏùë: ÏõåÏª§ Ïàò Ï†úÌïú (Í∏∞Ï°¥ *2 Ï†úÍ±∞)
                        phase1_worker_count = min(llm_max_workers, max(1, len(pending_codes)))
                        logger.info(f"   (LLM) Phase 1 ÏõåÏª§ Ïàò: {phase1_worker_count}Í∞ú (Rate Limit ÎåÄÏùë)")
                        
                        with ThreadPoolExecutor(max_workers=phase1_worker_count) as executor:
                            future_to_code = {}
                            for code in pending_codes:
                                payload = {
                                    'code': code,
                                    'info': candidate_stocks[code],
                                }
                                # [v4.2] Ï∫êÏãúÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Ï°∞ÌöåÌïòÎèÑÎ°ù Î≥ÄÍ≤Ω (API Ìò∏Ï∂ú X)
                                future = executor.submit(process_phase1_hunter_task, payload, brain, snapshot_cache, news_cache)
                                future_to_code[future] = code
                            
                            for future in as_completed(future_to_code):
                                result = future.result()
                                if result:
                                    phase1_results.append(result)
                                    # Phase 1 ÌÉàÎùΩ Ï¢ÖÎ™©ÎèÑ Í∏∞Î°ù (Ï∫êÏãúÏö©)
                                    if not result['passed']:
                                        llm_decision_records[result['code']] = {
                                            'code': result['code'],
                                            'name': result['name'],
                                            'is_tradable': False,
                                            'llm_score': result['hunter_score'],
                                            'llm_reason': result['hunter_reason'] or 'Phase 1 ÌïÑÌÑ∞ÎßÅ ÌÉàÎùΩ',
                                            'approved': False,
                                            'hunter_score': result['hunter_score'],  # [v4.3] Ï∫êÏãú Ï†ÄÏû•Ïö©
                                            'llm_metadata': {
                                                'llm_grade': 'D',
                                                'llm_updated_at': _utcnow().isoformat(),
                                                'source': 'llm_hunter_reject',
                                            }
                                        }
                        
                        phase1_passed_all = [r for r in phase1_results if r['passed']]
                        phase1_time = time.time() - phase1_start
                        logger.info(f"   (LLM) [Pass 1] Phase 1 ÏôÑÎ£å: {len(phase1_passed_all)}/{len(pending_codes)}Í∞ú ÌÜµÍ≥º ({phase1_time:.1f}Ï¥à)")
                        
                        # [v4.1] Phase 2 ÏßÑÏûÖ Ï†úÌïú: ÏÉÅÏúÑ 50Í∞úÎßå (ÏÜçÎèÑ ÏµúÏ†ÅÌôî)
                        PHASE2_MAX_ENTRIES = int(os.getenv("SCOUT_PHASE2_MAX_ENTRIES", "50"))
                        if len(phase1_passed_all) > PHASE2_MAX_ENTRIES:
                            phase1_passed_sorted = sorted(phase1_passed_all, key=lambda x: x['hunter_score'], reverse=True)
                            phase1_passed = phase1_passed_sorted[:PHASE2_MAX_ENTRIES]
                            logger.info(f"   (LLM) [ÏÜçÎèÑ ÏµúÏ†ÅÌôî] Phase 2 ÏßÑÏûÖ Ï†úÌïú: ÏÉÅÏúÑ {PHASE2_MAX_ENTRIES}Í∞úÎßå ÏÑ†ÌÉù (Ï†ÑÏ≤¥ {len(phase1_passed_all)}Í∞ú Ï§ë)")
                        else:
                            phase1_passed = phase1_passed_all
                        
                        # [v1.0] Redis ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ - Phase 1 ÏôÑÎ£å
                        update_pipeline_status(
                            phase=2, phase_name="Bull vs Bear Debate", status="running",
                            total_candidates=len(candidate_stocks),
                            passed_phase1=len(phase1_passed_all)  # Ï†ÑÏ≤¥ ÌÜµÍ≥º Ïàò ÌëúÏãú
                        )
                        
                        # Pass 2: Phase 2-3 Debate+Judge (GPT-5-mini) - ÏÉÅÏúÑ Ï¢ÖÎ™©Îßå
                        if phase1_passed:
                            logger.info(f"   (LLM) [Pass 2] Phase 2-3 Debate-Judge Ïã§Ìñâ ({len(phase1_passed)}Í∞ú Ï¢ÖÎ™©)")
                            phase23_start = time.time()
                            
                            phase23_worker_count = min(llm_max_workers, max(1, len(phase1_passed)))
                            
                            with ThreadPoolExecutor(max_workers=phase23_worker_count) as executor:
                                future_to_code = {}
                                for phase1_result in phase1_passed:
                                    future = executor.submit(process_phase23_debate_judge_task, phase1_result, brain)
                                    future_to_code[future] = phase1_result['code']
                                
                                for future in as_completed(future_to_code):
                                    record = future.result()
                                    if not record:
                                        continue
                                    llm_invocation_count += 1
                                    llm_decision_records[record['code']] = record
                                    if record.get('approved'):
                                        final_approved_list.append(_record_to_watchlist_entry(record))
                            
                            phase23_time = time.time() - phase23_start
                            logger.info(f"   (LLM) [Pass 2] Phase 2-3 ÏôÑÎ£å ({phase23_time:.1f}Ï¥à)")
                            
                            # [v1.0] Redis ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ - Phase 2-3 ÏôÑÎ£å
                            update_pipeline_status(
                                phase=3, phase_name="Final Judge", status="running",
                                total_candidates=len(candidate_stocks),
                                passed_phase1=len(phase1_passed),
                                passed_phase2=len(phase1_passed),  # DebateÏùÄ Ï†ÑÏõê Ï∞∏Ïó¨
                                final_selected=len(final_approved_list)
                            )
                        else:
                            logger.info("   (LLM) [Pass 2] Phase 1 ÌÜµÍ≥º Ï¢ÖÎ™© ÏóÜÏùå, Phase 2-3 Í±¥ÎÑàÎúÄ")
                else:
                    logger.info("   (LLM) Î™®Îì† ÌõÑÎ≥¥Í∞Ä Ï∫êÏãúÎ°ú Ï∂©Ï°±ÎêòÏñ¥ Ïã†Í∑ú Ìò∏Ï∂úÏù¥ ÏóÜÏäµÎãàÎã§.")
    
                logger.info("   (LLM) Ïã†Í∑ú Ìò∏Ï∂ú Ïàò: %d", llm_invocation_count)
    
                # [v4.3] ÏÉàÎ°úÏö¥ Ï∫êÏãú ÌÖåÏù¥Î∏îÏóê Í≤∞Í≥º Ï†ÄÏû•
                if llm_invocation_count > 0:
                    new_cache_entries = {}
                    for code, record in llm_decision_records.items():
                        info = candidate_stocks.get(code, {})
                        new_cache_entries[code] = {
                            'stock_name': record.get('name', ''),
                            'price_bucket': _get_price_bucket(info.get('price', 0)),
                            'volume_bucket': _get_volume_bucket(info.get('volume', 0)),
                            'news_hash': info.get('news_hash'),
                            'eval_date': today_str,
                            'hunter_score': record.get('hunter_score', record.get('llm_score', 0)),
                            'judge_score': record.get('llm_score', 0),
                            'llm_grade': record.get('llm_metadata', {}).get('llm_grade'),
                            'llm_reason': record.get('llm_reason', '')[:60000] if record.get('llm_reason') else None,
                            'news_used': news_cache.get(code, '')[:60000] if news_cache.get(code) else None,
                            'is_approved': record.get('approved', False),
                            'is_tradable': record.get('is_tradable', False),
                        }
                    _save_llm_cache_batch(session, new_cache_entries)
                    _save_last_llm_run_at(session, _utcnow())
    
                # [v1.0] Phase 3: ÏøºÌÑ∞Ï†ú Ï†ÅÏö© (Top 15Í∞úÎßå Ï†ÄÏû•) - Ï†úÎãà ÌîºÎìúÎ∞± Î∞òÏòÅ
                MAX_WATCHLIST_SIZE = 15
                
                # Ï†êÏàò Í∏∞Ï§Ä ÎÇ¥Î¶ºÏ∞®Ïàú Ï†ïÎ†¨ ÌõÑ ÏÉÅÏúÑ NÍ∞úÎßå ÏÑ†ÌÉù
                if len(final_approved_list) > MAX_WATCHLIST_SIZE:
                    final_approved_list_sorted = sorted(
                        final_approved_list, 
                        key=lambda x: x.get('llm_score', 0), 
                        reverse=True
                    )
                    final_approved_list = final_approved_list_sorted[:MAX_WATCHLIST_SIZE]
                    logger.info(f"   (ÏøºÌÑ∞Ï†ú) ÏÉÅÏúÑ {MAX_WATCHLIST_SIZE}Í∞úÎßå ÏÑ†Ï†ï (Ï¥ù {len(final_approved_list_sorted)}Í∞ú Ï§ë)")
            
            # =============================================================
            # [Í≥µÌÜµ] Phase 3: ÏµúÏ¢Ö Watchlist Ï†ÄÏû•
            # =============================================================
            logger.info(f"--- [Phase 3] ÏµúÏ¢Ö Watchlist {len(final_approved_list)}Í∞ú Ï†ÄÏû• ---")
            database.save_to_watchlist(session, final_approved_list)
            
            with ThreadPoolExecutor(max_workers=10) as executor:
                if hasattr(kis_api, 'API_CALL_DELAY'):
                    future_to_data = {
                        executor.submit(fetch_kis_data_task, s, kis_api): (time.sleep(kis_api.API_CALL_DELAY), s)[1]
                        for s in final_approved_list 
                    }
                else:
                    future_to_data = {
                        executor.submit(fetch_kis_data_task, s, kis_api): s
                        for s in final_approved_list 
                    }
                
                all_daily = []
                all_fund = []
                for future in as_completed(future_to_data):
                    d, f = future.result()
                    if d: all_daily.extend(d)
                    if f: all_fund.append(f)
            
            if all_daily: database.save_all_daily_prices(session, all_daily)
            if all_fund: database.update_all_stock_fundamentals(session, all_fund)
            
            # Phase 3-A: Ïû¨Î¨¥ Îç∞Ïù¥ÌÑ∞ (ÎÑ§Ïù¥Î≤Ñ ÌÅ¨Î°§ÎßÅ)
            tradable_codes = [s['code'] for s in final_approved_list if s.get('is_tradable', True)]
            if tradable_codes:
                batch_update_watchlist_financial_data(session, tradable_codes)
            
            # [v1.0] Redis ÏµúÏ¢Ö ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏ - ÏôÑÎ£å
            update_pipeline_status(
                phase=3, phase_name="Final Judge", status="completed",
                progress=100,
                total_candidates=len(candidate_stocks) if 'candidate_stocks' in locals() else 0,
                passed_phase1=len(phase1_passed) if 'phase1_passed' in locals() else 0,
                passed_phase2=len(phase1_passed) if 'phase1_passed' in locals() else 0,
                final_selected=len(final_approved_list)
            )
            
            # [v1.0] Redis Í≤∞Í≥º Ï†ÄÏû• (DashboardÏóêÏÑú Ï°∞ÌöåÏö©)
            pipeline_results = [
                {
                    "stock_code": s.get('code'),
                    "stock_name": s.get('name'),
                    "grade": s.get('llm_metadata', {}).get('llm_grade', 'C'),
                    "final_score": s.get('llm_score', 0),
                    "selected": s.get('approved', False),
                    "judge_reason": s.get('llm_reason', ''),
                }
                for s in final_approved_list
            ]
            save_pipeline_results(pipeline_results)
            logger.info(f"   (Redis) DashboardÏö© Í≤∞Í≥º Ï†ÄÏû• ÏôÑÎ£å ({len(pipeline_results)}Í∞ú)")

    except Exception as e:
        logger.critical(f"‚ùå 'Scout Job' Ïã§Ìñâ Ï§ë Ïò§Î•ò: {e}", exc_info=True)
        # [v1.0] Ïò§Î•ò Ïãú Redis ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
        update_pipeline_status(phase=0, phase_name="Error", status="error")
            
    logger.info(f"--- ü§ñ 'Scout Job' Ï¢ÖÎ£å (ÏÜåÏöî: {time.time() - start_time:.2f}Ï¥à) ---")

if __name__ == "__main__":
    main()
