#!/usr/bin/env python3
# Version: v1.0
# ì‘ì—… LLM: Claude Sonnet 4.5, Claude Opus 4.5
"""
Scout Job v1.0 - ì¢…ëª© ë°œêµ´ íŒŒì´í”„ë¼ì¸
- ê¹ê¹í•œ í•„í„°ë§ (ê¸°ë³¸ì ìˆ˜ 20, Hunter í†µê³¼ 60ì , Judge ìŠ¹ì¸ 75ì )
- [v1.0] ì¿¼í„°ì œ ë„ì…: ìµœì¢… Watchlist ìƒìœ„ 15ê°œë§Œ ì €ì¥
- [v1.0] Debate í”„ë¡¬í”„íŠ¸ ê°•í™”: Bull/Bear ìºë¦­í„° ê·¹ë‹¨ì ìœ¼ë¡œ ì„¤ì •
- Redis ìƒíƒœ ì €ì¥: Dashboardì—ì„œ ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ ì§„í–‰ ìƒí™© í™•ì¸ ê°€ëŠ¥
- ê²½ìŸì‚¬ ìˆ˜í˜œ ì ìˆ˜ ë°˜ì˜: ê²½ìŸì‚¬ ì•…ì¬ ì‹œ Hunter ì ìˆ˜ì— ê°€ì‚°
"""

import logging
import os
import sys
import time
import re
import threading
import json
import hashlib
from typing import Dict, Tuple, List, Optional
from datetime import datetime, timezone, timedelta
from dotenv import load_dotenv
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import redis

# ë¡œê¹… ì„¤ì •ì„ ëª¨ë“  import ë³´ë‹¤ ë¨¼ì € ìˆ˜í–‰
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s] - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)

logger = logging.getLogger(__name__)

# ê³µìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ë¥¼ ìœ„í•œ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))  # /app
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

import shared.auth as auth
import shared.database as database
from shared.db.connection import session_scope, ensure_engine_initialized
from shared.kis import KISClient as KIS_API
from shared.kis.gateway_client import KISGatewayClient
from shared.llm import JennieBrain
from shared.financial_data_collector import batch_update_watchlist_financial_data
from shared.gemini import ensure_gemini_api_key  # [v3.0] Local Gemini Auth ì¶”ê°€

import chromadb
from langchain_chroma import Chroma
# from langchain_google_vertexai import VertexAIEmbeddings # [v3.0] Vertex AI ì œê±°
from langchain_google_genai import GoogleGenerativeAIEmbeddings # [v3.0] Gemini API Key ê¸°ë°˜

# [v3.8] FinanceDataReader for KOSPI 200 Universe
try:
    import FinanceDataReader as fdr
    FDR_AVAILABLE = True
    logger.info("âœ… FinanceDataReader ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError:
    FDR_AVAILABLE = False
    logger.warning("âš ï¸ FinanceDataReader ë¯¸ì„¤ì¹˜ - ë„¤ì´ë²„ ê¸ˆìœµ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ í´ë°±")

# [v2.2 ìˆ˜ì •] backtest ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from utilities.backtest import Backtester
    logger.info("âœ… Backtester ëª¨ë“ˆ ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    logger.warning(f"âš ï¸ Backtester ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨ (ë°±í…ŒìŠ¤íŠ¸ ê¸°ëŠ¥ ë¹„í™œì„±í™”): {e}")
    Backtester = None

# Chroma ì„œë²„
CHROMA_SERVER_HOST = os.getenv("CHROMA_SERVER_HOST", "10.178.0.2") 
CHROMA_SERVER_PORT = 8000

# --- (B) ì •ì  ìš°ëŸ‰ì£¼ ëª©ë¡ (ì•ˆì „ë§/Fallback) ---
BLUE_CHIP_STOCKS = [
    {"code": "0001", "name": "KOSPI", "is_tradable": False},
    {"code": "005930", "name": "ì‚¼ì„±ì „ì", "is_tradable": True},
    # ... (ì´í•˜ ìƒëµ, ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ ìœ ì§€)
    {"code": "000660", "name": "SKí•˜ì´ë‹‰ìŠ¤", "is_tradable": True},
    {"code": "035420", "name": "NAVER", "is_tradable": True},
    {"code": "035720", "name": "ì¹´ì¹´ì˜¤", "is_tradable": True},
]

# =============================================================================
# [v1.1 Refactored] ìºì‹œ/ìƒíƒœ ê´€ë¦¬ í•¨ìˆ˜ë“¤ì€ scout_cache.pyë¡œ ë¶„ë¦¬ë¨
# =============================================================================
from scout_cache import (
    # ìƒìˆ˜
    STATE_PREFIX, CANDIDATE_DIGEST_SUFFIX, CANDIDATE_HASHES_SUFFIX,
    LLM_CACHE_SUFFIX, LLM_LAST_RUN_SUFFIX, ISO_FORMAT_Z,
    REDIS_URL,
    # Redis í•¨ìˆ˜
    _get_redis, _utcnow, update_pipeline_status, save_pipeline_results,
    # CONFIG í…Œì´ë¸” í•¨ìˆ˜
    _get_scope, _make_state_key, _load_json_config, _save_json_config,
    _get_last_llm_run_at, _save_last_llm_run_at,
    _load_candidate_state, _save_candidate_state,
    _load_llm_cache, _save_llm_cache,
    # LLM_EVAL_CACHE í…Œì´ë¸” í•¨ìˆ˜
    _load_llm_cache_from_db, _save_llm_cache_to_db, _save_llm_cache_batch,
    # ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬ ë° í•´ì‹œ ê³„ì‚°
    _is_cache_valid_direct, _get_price_bucket, _get_volume_bucket, _get_foreign_direction,
    _hash_candidate_payload, _compute_candidate_hashes,
    _minutes_since, _parse_int_env, _is_cache_entry_valid,
    _record_to_watchlist_entry, _record_to_cache_payload, _cache_payload_to_record,
)

# =============================================================================
# [v1.1 Refactored] ì¢…ëª© ìœ ë‹ˆë²„ìŠ¤ ê´€ë ¨ í•¨ìˆ˜ë“¤ì€ scout_universe.pyë¡œ ë¶„ë¦¬ë¨
# =============================================================================
from scout_universe import (
    SECTOR_MAPPING, BLUE_CHIP_STOCKS, FDR_AVAILABLE,
    analyze_sector_momentum, get_hot_sector_stocks,
    get_dynamic_blue_chips, get_momentum_stocks,
)

# =============================================================================
# [v1.1 Refactored] ìë™ ìµœì í™” í•¨ìˆ˜ë“¤ì€ scout_optimizer.pyë¡œ ë¶„ë¦¬ë¨
# =============================================================================
from scout_optimizer import (
    run_auto_parameter_optimization,
    run_simple_backtest, generate_optimized_params, verify_params_with_llm,
)

# =============================================================================
# [v1.1 Refactored] íŒŒì´í”„ë¼ì¸ íƒœìŠ¤í¬ í•¨ìˆ˜ë“¤ì€ scout_pipeline.pyë¡œ ë¶„ë¦¬ë¨
# =============================================================================
from scout_pipeline import (
    is_hybrid_scoring_enabled,
    process_quant_scoring_task,
    process_phase1_hunter_v5_task, process_phase23_judge_v5_task,
    process_phase1_hunter_task, process_phase23_debate_judge_task,
    process_llm_decision_task, fetch_kis_data_task,
)

_redis_client = None  # scout_cacheì—ì„œ ê´€ë¦¬í•˜ì§€ë§Œ í˜¸í™˜ì„± ìœ ì§€




def prefetch_all_data(candidate_stocks: Dict[str, Dict], kis_api, vectorstore) -> Tuple[Dict[str, Dict], Dict[str, str]]:
    """
    [v4.2] Phase 1 ì‹œì‘ ì „ì— ëª¨ë“  ë°ì´í„°ë¥¼ ì¼ê´„ ì¡°íšŒí•˜ì—¬ ìºì‹œ
    
    Returns:
        (snapshot_cache, news_cache) - ì¢…ëª©ì½”ë“œë¥¼ í‚¤ë¡œ í•˜ëŠ” dict
    
    íš¨ê³¼: ë³‘ë ¬ ìŠ¤ë ˆë“œ ì•ˆì—ì„œ API í˜¸ì¶œ ì œê±° â†’ Rate Limit íšŒí”¼ + ì†ë„ í–¥ìƒ
    """
    stock_codes = list(candidate_stocks.keys())
    logger.info(f"   (Prefetch) {len(stock_codes)}ê°œ ì¢…ëª© ë°ì´í„° ì‚¬ì „ ì¡°íšŒ ì‹œì‘...")
    
    snapshot_cache: Dict[str, Dict] = {}
    news_cache: Dict[str, str] = {}
    
    prefetch_start = time.time()
    
    # 1. KIS API ìŠ¤ëƒ…ìƒ· ë³‘ë ¬ ì¡°íšŒ (4ê°œ ì›Œì»¤)
    logger.info(f"   (Prefetch) KIS ìŠ¤ëƒ…ìƒ· ì¡°íšŒ ì¤‘...")
    snapshot_start = time.time()
    
    def fetch_snapshot(code):
        try:
            if hasattr(kis_api, 'API_CALL_DELAY'):
                time.sleep(kis_api.API_CALL_DELAY * 0.3)  # ì•½ê°„ì˜ ë”œë ˆì´
            return code, kis_api.get_stock_snapshot(code)
        except Exception as e:
            logger.debug(f"   âš ï¸ [{code}] Snapshot ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return code, None
    
    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = [executor.submit(fetch_snapshot, code) for code in stock_codes]
        for future in as_completed(futures):
            code, snapshot = future.result()
            if snapshot:
                snapshot_cache[code] = snapshot
    
    snapshot_time = time.time() - snapshot_start
    logger.info(f"   (Prefetch) âœ… KIS ìŠ¤ëƒ…ìƒ· {len(snapshot_cache)}/{len(stock_codes)}ê°œ ì¡°íšŒ ì™„ë£Œ ({snapshot_time:.1f}ì´ˆ)")
    
    # 2. ChromaDB ë‰´ìŠ¤ ë³‘ë ¬ ì¡°íšŒ (8ê°œ ì›Œì»¤)
    if vectorstore:
        logger.info(f"   (Prefetch) ChromaDB ë‰´ìŠ¤ ì¡°íšŒ ì¤‘...")
        news_start = time.time()
        
        def fetch_news(code_name):
            code, name = code_name
            try:
                news = fetch_stock_news_from_chroma(vectorstore, code, name, k=3)
                return code, news
            except Exception as e:
                logger.debug(f"   âš ï¸ [{code}] ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                return code, "ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨"
        
        code_name_pairs = [(code, info.get('name', '')) for code, info in candidate_stocks.items()]
        
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = [executor.submit(fetch_news, pair) for pair in code_name_pairs]
            for future in as_completed(futures):
                code, news = future.result()
                news_cache[code] = news
        
        news_time = time.time() - news_start
        valid_news = sum(1 for n in news_cache.values() if n and n not in ["ë‰´ìŠ¤ DB ë¯¸ì—°ê²°", "ìµœê·¼ ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ", "ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜", "ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨"])
        logger.info(f"   (Prefetch) âœ… ChromaDB ë‰´ìŠ¤ {valid_news}/{len(stock_codes)}ê°œ ì¡°íšŒ ì™„ë£Œ ({news_time:.1f}ì´ˆ)")
    
    total_time = time.time() - prefetch_start
    logger.info(f"   (Prefetch) âœ… ì „ì²´ ì‚¬ì „ ì¡°íšŒ ì™„ë£Œ ({total_time:.1f}ì´ˆ)")
    
    return snapshot_cache, news_cache


def enrich_candidates_with_market_data(candidate_stocks: Dict[str, Dict], session, vectorstore) -> None:
    """
    [v4.1] í›„ë³´êµ°ì— ì‹œì¥ ë°ì´í„° ì¶”ê°€ (í•´ì‹œ ê³„ì‚°ìš©)
    
    í•´ì‹œì— í¬í•¨ë  ë°ì´í„°:
    - price: ìµœì‹  ì¢…ê°€ (5% ë²„í‚·í™”ë¨)
    - volume: ìµœì‹  ê±°ë˜ëŸ‰ (10ë§Œì£¼ ë²„í‚·í™”ë¨)
    - foreign_net: ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ (ë°©í–¥ë§Œ - buy/sell/neutral)
    - news_date: ìµœì‹  ë‰´ìŠ¤ ë‚ ì§œ (YYYY-MM-DD)
    """
    if not candidate_stocks:
        return
    
    stock_codes = list(candidate_stocks.keys())
    logger.info(f"   (Hash) {len(stock_codes)}ê°œ ì¢…ëª© ì‹œì¥ ë°ì´í„° ì¡°íšŒ ì¤‘...")
    
    # 1. DBì—ì„œ ìµœì‹  ê°€ê²©/ê±°ë˜ëŸ‰ ë°ì´í„° ì¼ê´„ ì¡°íšŒ
    try:
        from sqlalchemy import text
        
        placeholders = ','.join([f"'{code}'" for code in stock_codes])
        
        # ìµœì‹  ë‚ ì§œì˜ ë°ì´í„°ë§Œ ì¡°íšŒ (ê°€ê²©, ê±°ë˜ëŸ‰)
        query = text(f"""
            SELECT STOCK_CODE, CLOSE_PRICE, VOLUME, PRICE_DATE
            FROM STOCK_DAILY_PRICES_3Y
            WHERE STOCK_CODE IN ({placeholders})
            AND (STOCK_CODE, PRICE_DATE) IN (
                SELECT STOCK_CODE, MAX(PRICE_DATE) 
                FROM STOCK_DAILY_PRICES_3Y
                WHERE STOCK_CODE IN ({placeholders})
                GROUP BY STOCK_CODE
            )
        """)
        rows = session.execute(query).fetchall()
        
        for row in rows:
            code = row[0]
            price = row[1]
            volume = row[2]
            
            if code in candidate_stocks:
                candidate_stocks[code]['price'] = float(price) if price else 0
                candidate_stocks[code]['volume'] = int(volume) if volume else 0
        
        logger.info(f"   (Hash) âœ… DBì—ì„œ {len(rows)}ê°œ ì¢…ëª© ì‹œì¥ ë°ì´í„° ë¡œë“œ")
    except Exception as e:
        logger.warning(f"   (Hash) âš ï¸ DB ì‹œì¥ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    # 2. ChromaDB ë‰´ìŠ¤ ì¡°íšŒ ìƒëµ (ì†ë„ ìµœì í™”)
    # ì´ìœ : í•´ì‹œì— ì˜¤ëŠ˜ ë‚ ì§œê°€ í¬í•¨ë˜ì–´ ìˆì–´ì„œ ë§¤ì¼ ì¬í‰ê°€ ë³´ì¥ë¨
    # ë‰´ìŠ¤ ë°ì´í„°ëŠ” Phase 1 Hunterì—ì„œ ê°œë³„ ì¢…ëª© í‰ê°€ ì‹œ ì¡°íšŒí•¨
    logger.info(f"   (Hash) âœ… ë‰´ìŠ¤ ë‚ ì§œ ì¡°íšŒ ìƒëµ (ë‚ ì§œ ê¸°ë°˜ ìºì‹œ ë¬´íš¨í™”ë¡œ ëŒ€ì²´)")


def _get_latest_news_date(vectorstore, stock_code: str, stock_name: str) -> Optional[str]:
    """ChromaDBì—ì„œ ì¢…ëª©ì˜ ìµœì‹  ë‰´ìŠ¤ ë‚ ì§œ ì¡°íšŒ"""
    try:
        docs = vectorstore.similarity_search(
            query=f"{stock_name}",
            k=1,
            filter={"stock_code": stock_code}
        )
        if docs and docs[0].metadata:
            # ë‰´ìŠ¤ ë‚ ì§œë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
            news_date = docs[0].metadata.get('date') or docs[0].metadata.get('published_at')
            if news_date:
                # ë‚ ì§œ ë¬¸ìì—´ì—ì„œ YYYY-MM-DDë§Œ ì¶”ì¶œ
                return str(news_date)[:10]
    except Exception:
        pass
    return None


def _record_to_cache_payload(record: Dict) -> Dict:
    metadata = record.get("llm_metadata", {})
    return {
        "code": record["code"],
        "name": record["name"],
        "llm_score": record.get("llm_score", 0),
        "llm_reason": record.get("llm_reason", ""),
        "llm_grade": metadata.get("llm_grade"),
        "decision_hash": metadata.get("decision_hash"),
        "llm_updated_at": metadata.get("llm_updated_at"),
        "is_tradable": record.get("is_tradable", True),
        "approved": record.get("approved", False),
    }


def _cache_payload_to_record(entry: Dict, decision_hash: str) -> Dict:
    updated_at = entry.get("llm_updated_at")
    metadata = {
        "llm_grade": entry.get("llm_grade"),
        "decision_hash": decision_hash,
        "llm_updated_at": updated_at,
        "source": "cache",
    }
    return {
        "code": entry["code"],
        "name": entry.get("name", entry["code"]),
        "llm_score": entry.get("llm_score", 0),
        "llm_reason": entry.get("llm_reason", ""),
        "is_tradable": entry.get("is_tradable", True),
        "approved": entry.get("approved", False),
        "llm_metadata": metadata,
    }

# ì„¹í„°/í…Œë§ˆ ë¶„ì„ í•¨ìˆ˜ë“¤ì€ scout_universe.pyì—ì„œ importë¨
# (analyze_sector_momentum, get_hot_sector_stocks, get_dynamic_blue_chips, get_momentum_stocks)

# ìë™ íŒŒë¼ë¯¸í„° ìµœì í™” í•¨ìˆ˜ë“¤ì€ scout_optimizer.pyì—ì„œ importë¨
# (run_auto_parameter_optimization, run_simple_backtest, generate_optimized_params, verify_params_with_llm)


def fetch_stock_news_from_chroma(vectorstore, stock_code: str, stock_name: str, k: int = 3) -> str:
    """
    [v3.9] ChromaDBì—ì„œ ì¢…ëª©ë³„ ìµœì‹  ë‰´ìŠ¤ ê²€ìƒ‰
    
    Args:
        vectorstore: ChromaDB vectorstore ì¸ìŠ¤í„´ìŠ¤
        stock_code: ì¢…ëª© ì½”ë“œ
        stock_name: ì¢…ëª©ëª…
        k: ê°€ì ¸ì˜¬ ë‰´ìŠ¤ ê°œìˆ˜
        
    Returns:
        ë‰´ìŠ¤ ìš”ì•½ ë¬¸ìì—´ (ì—†ìœ¼ë©´ "ìµœê·¼ ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ")
    """
    if not vectorstore:
        return "ë‰´ìŠ¤ DB ë¯¸ì—°ê²°"
    
    try:
        from datetime import datetime, timedelta, timezone
        
        # ìµœì‹  7ì¼ ì´ë‚´ ë‰´ìŠ¤ í•„í„°
        recency_timestamp = int((datetime.now(timezone.utc) - timedelta(days=7)).timestamp())
        
        # ì¢…ëª© ì½”ë“œë¡œ í•„í„°ë§ëœ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œë„
        try:
            docs = vectorstore.similarity_search(
                query=f"{stock_name} ì‹¤ì  ìˆ˜ì£¼ í˜¸ì¬",
                k=k,
                filter={"stock_code": stock_code}
            )
            # logger.debug(f"   (D) [{stock_code}] í•„í„° ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê±´")
        except Exception:
            # í•„í„° ì‹¤íŒ¨ì‹œ ì¢…ëª©ëª…ìœ¼ë¡œ ê²€ìƒ‰
            docs = vectorstore.similarity_search(
                query=f"{stock_name} ì£¼ì‹ ë‰´ìŠ¤",
                k=k
            )
            logger.debug(f"   (D) [{stock_code}] ì¢…ëª©ëª… ê²€ìƒ‰(Fallback): {len(docs)}ê±´")
            # ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ë§Œ í•„í„°ë§
            docs = [d for d in docs if stock_name in d.page_content or stock_code in str(d.metadata)]
        
        if docs:
            news_items = []
            for i, doc in enumerate(docs[:k], 1):
                content = doc.page_content[:100].strip()
                if content:
                    news_items.append(f"[ë‰´ìŠ¤{i}] {content}")
            
            if news_items:
                return " | ".join(news_items)
        
        return "ìµœê·¼ ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ"
        
    except Exception as e:
        logger.debug(f"   âš ï¸ [{stock_code}] ChromaDB ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return "ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜"


# =============================================================================
# [v1.0 Refactored] íŒŒì´í”„ë¼ì¸ íƒœìŠ¤í¬ í•¨ìˆ˜ë“¤ì€ scout_pipeline.pyë¡œ ë¶„ë¦¬ë¨
# - is_hybrid_scoring_enabled, process_quant_scoring_task
# - process_phase1_hunter_v5_task, process_phase23_judge_v5_task
# - process_phase1_hunter_task, process_phase23_debate_judge_task
# - process_llm_decision_task, fetch_kis_data_task
# =============================================================================

def main():
    start_time = time.time()
    logger.info("--- ğŸ¤– 'Scout Job' [v3.0 Local] ì‹¤í–‰ ì‹œì‘ ---")
    
    kis_api = None
    brain = None

    try:
        logger.info("--- [Init] í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° KIS API ì—°ê²° ì‹œì‘ ---")
        load_dotenv(override=True)
        
        trading_mode = os.getenv("TRADING_MODE", "REAL")
        use_gateway = os.getenv("USE_KIS_GATEWAY", "true").lower() == "true"
        
        if use_gateway:
            kis_api = KISGatewayClient()
            logger.info("âœ… KIS Gateway Client ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            kis_api = KIS_API(
                app_key=auth.get_secret(os.getenv(f"{trading_mode}_SECRET_ID_APP_KEY")),
                app_secret=auth.get_secret(os.getenv(f"{trading_mode}_SECRET_ID_APP_SECRET")),
                base_url=os.getenv(f"KIS_BASE_URL_{trading_mode}"),
                account_prefix=auth.get_secret(os.getenv(f"{trading_mode}_SECRET_ID_ACCOUNT_PREFIX")),
                account_suffix=os.getenv("KIS_ACCOUNT_SUFFIX"),
                token_file_path="/app/tokens/kis_token_scout.json",
                trading_mode=trading_mode
            )
            if not kis_api.authenticate():
                raise Exception("KIS API ì¸ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        brain = JennieBrain(
            project_id=os.getenv("GCP_PROJECT_ID", "local"),
            gemini_api_key_secret=os.getenv("SECRET_ID_GEMINI_API_KEY")
        )
        
        # [v4.3] SQLAlchemy ì„¸ì…˜ ì´ˆê¸°í™” (session_scope ì‚¬ìš© ì „ì— í˜¸ì¶œ í•„ìˆ˜)
        ensure_engine_initialized()
        
        # [v4.3] SQLAlchemy ì„¸ì…˜ ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½
        with session_scope() as session:
            watchlist_snapshot = database.get_active_watchlist(session)
            
            vectorstore = None
            try:
                logger.info("   ... ChromaDB í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹œë„ (Gemini Embeddings) ...")
                api_key = ensure_gemini_api_key()
                embeddings = GoogleGenerativeAIEmbeddings(
                    model="models/gemini-embedding-001", 
                    google_api_key=api_key
                )
                
                chroma_client = chromadb.HttpClient( # noqa
                    host=CHROMA_SERVER_HOST, 
                    port=CHROMA_SERVER_PORT
                )
                vectorstore = Chroma(
                    client=chroma_client, 
                    collection_name="rag_stock_data", 
                    embedding_function=embeddings
                )
                logger.info("âœ… [v3.0] LLM ë° ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.")
            except Exception as e:
                logger.warning(f"âš ï¸ ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨ (RAG ê¸°ëŠ¥ ë¹„í™œì„±í™”): {e}")
                vectorstore = None

            # Phase 1: íŠ¸ë¦¬í”Œ ì†ŒìŠ¤ í›„ë³´ ë°œêµ´ (v3.8: ì„¹í„° ë¶„ì„ ì¶”ê°€)
            logger.info("--- [Phase 1] íŠ¸ë¦¬í”Œ ì†ŒìŠ¤ í›„ë³´ ë°œêµ´ ì‹œì‘ ---")
            update_pipeline_status(phase=1, phase_name="Hunter Scout", status="running", progress=0)
            candidate_stocks = {}

            # A: ë™ì  ìš°ëŸ‰ì£¼ (KOSPI 200 ê¸°ì¤€)
            universe_size = int(os.getenv("SCOUT_UNIVERSE_SIZE", "200"))
            for stock in get_dynamic_blue_chips(limit=universe_size):
                candidate_stocks[stock['code']] = {'name': stock['name'], 'reasons': ['KOSPI ì‹œì´ ìƒìœ„']}
            
            # E: ì„¹í„° ëª¨ë©˜í…€ ë¶„ì„ (v3.8 ì‹ ê·œ)
            sector_analysis = analyze_sector_momentum(kis_api, session, watchlist_snapshot)
            hot_sector_stocks = get_hot_sector_stocks(sector_analysis, top_n=30)
            for stock in hot_sector_stocks:
                if stock['code'] not in candidate_stocks:
                    candidate_stocks[stock['code']] = {
                        'name': stock['name'], 
                        'reasons': [f"í•« ì„¹í„° ({stock['sector']}, +{stock['sector_momentum']:.1f}%)"]
                    }
                else:
                    candidate_stocks[stock['code']]['reasons'].append(
                        f"í•« ì„¹í„° ({stock['sector']}, +{stock['sector_momentum']:.1f}%)"
                    )

            # B: ì •ì  ìš°ëŸ‰ì£¼
            for stock in BLUE_CHIP_STOCKS:
                if stock['code'] not in candidate_stocks:
                    candidate_stocks[stock['code']] = {'name': stock['name'], 'reasons': ['ì •ì  ìš°ëŸ‰ì£¼']}

            # C: RAG
            if vectorstore:
                try:
                    logger.info("   (C) RAG ê¸°ë°˜ í›„ë³´ ë°œêµ´ ì¤‘...")
                    rag_results = vectorstore.similarity_search(query="ì‹¤ì  í˜¸ì¬ ê³„ì•½ ìˆ˜ì£¼", k=50)
                    for doc in rag_results:
                        stock_code = doc.metadata.get('stock_code')
                        stock_name = doc.metadata.get('stock_name')
                        if stock_code and stock_name:
                            if stock_code not in candidate_stocks:
                                candidate_stocks[stock_code] = {'name': stock_name, 'reasons': []}
                            candidate_stocks[stock_code]['reasons'].append(f"RAG í¬ì°©: {doc.page_content[:20]}...")
                except Exception as e:
                    logger.warning(f"   (C) RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

            # D: ëª¨ë©˜í…€
            logger.info("   (D) ëª¨ë©˜í…€ íŒ©í„° ê¸°ë°˜ ì¢…ëª© ë°œêµ´ ì¤‘...")
            momentum_stocks = get_momentum_stocks(
                    kis_api,
                    session,
                period_months=6,
                top_n=30,
                watchlist_snapshot=watchlist_snapshot
            )
            for stock in momentum_stocks:
                if stock['code'] not in candidate_stocks:
                    candidate_stocks[stock['code']] = {
                        'name': stock['name'], 
                        'reasons': [f'ëª¨ë©˜í…€ ({stock["momentum"]:.1f}%)']
                    }
            
            logger.info(f"   âœ… í›„ë³´êµ° {len(candidate_stocks)}ê°œ ë°œêµ´ ì™„ë£Œ.")

            # [v4.1] í•´ì‹œ ê³„ì‚° ì „ì— ì‹œì¥ ë°ì´í„° ì¶”ê°€ (ê°€ê²©, ê±°ë˜ëŸ‰)
            logger.info("--- [Phase 1.5] ì‹œì¥ ë°ì´í„° ê¸°ë°˜ í•´ì‹œ ê³„ì‚° ---")
            enrich_candidates_with_market_data(candidate_stocks, session, vectorstore)
            
            # [v4.2] Phase 1 ì‹œì‘ ì „ì— ëª¨ë“  ë°ì´í„° ì¼ê´„ ì¡°íšŒ (ë³‘ë ¬ ìŠ¤ë ˆë“œ ì•ˆ API í˜¸ì¶œ ì œê±°)
            logger.info("--- [Phase 1.6] ë°ì´í„° ì‚¬ì „ ì¡°íšŒ (ìŠ¤ëƒ…ìƒ·/ë‰´ìŠ¤) ---")
            snapshot_cache, news_cache = prefetch_all_data(candidate_stocks, kis_api, vectorstore)

            # [v4.3] ë‰´ìŠ¤ í•´ì‹œë¥¼ candidate_stocksì— ë°˜ì˜ (í•´ì‹œ ê³„ì‚°ì— í¬í•¨)
            # ë‰´ìŠ¤ ë‚´ìš©ì´ ë°”ë€Œë©´ í•´ì‹œê°€ ë‹¬ë¼ì ¸ LLM ì¬í˜¸ì¶œë¨
            news_hash_count = 0
            for code, news in news_cache.items():
                if code in candidate_stocks and news and news not in [
                    "ë‰´ìŠ¤ DB ë¯¸ì—°ê²°", "ìµœê·¼ ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ", "ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜", 
                    "ë‰´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨", "ë‰´ìŠ¤ ìºì‹œ ì—†ìŒ"
                ]:
                    # ë‰´ìŠ¤ ë‚´ìš©ì˜ MD5 í•´ì‹œ (ì‹œê°„ ì •ë³´ í¬í•¨ë˜ì–´ ìˆìŒ)
                    candidate_stocks[code]['news_hash'] = hashlib.md5(news.encode()).hexdigest()[:16]
                    news_hash_count += 1
            logger.info(f"   (Hash) âœ… ë‰´ìŠ¤ í•´ì‹œ {news_hash_count}ê°œ ë°˜ì˜ ì™„ë£Œ")

            # Phase 2: LLM ìµœì¢… ì„ ì •
            logger.info("--- [Phase 2] LLM ê¸°ë°˜ ìµœì¢… Watchlist ì„ ì • ì‹œì‘ ---")
            update_pipeline_status(
                phase=1, phase_name="Hunter Scout", status="running", 
                total_candidates=len(candidate_stocks)
            )
            
            # =============================================================
            # [v1.0] í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§ ëª¨ë“œ ë¶„ê¸°
            # =============================================================
            if is_hybrid_scoring_enabled():
                logger.info("=" * 60)
                logger.info("   ğŸš€ Scout v5 Hybrid Scoring Mode í™œì„±í™”!")
                logger.info("=" * 60)
                
                try:
                    from shared.hybrid_scoring import (
                        QuantScorer, HybridScorer, 
                        create_hybrid_scoring_tables,
                        format_quant_score_for_prompt,
                    )
                    from shared.market_regime import MarketRegimeDetector
                    
                    # DB í…Œì´ë¸” ìƒì„± í™•ì¸
                    create_hybrid_scoring_tables(session)
                    
                    # ì‹œì¥ êµ­ë©´ ê°ì§€
                    kospi_prices = database.get_daily_prices(session, "0001", limit=60)
                    if not kospi_prices.empty:
                        detector = MarketRegimeDetector()
                        current_regime, _ = detector.detect_regime(kospi_prices, float(kospi_prices['CLOSE_PRICE'].iloc[-1]), quiet=True)
                    else:
                        current_regime = "SIDEWAYS"
                    
                    logger.info(f"   í˜„ì¬ ì‹œì¥ êµ­ë©´: {current_regime}")
                    
                    # QuantScorer ì´ˆê¸°í™”
                    quant_scorer = QuantScorer(session, market_regime=current_regime)
                    
                    # Step 1: ì •ëŸ‰ ì ìˆ˜ ê³„ì‚° (LLM í˜¸ì¶œ ì—†ìŒ, ë¹„ìš© 0ì›)
                    logger.info(f"\n   [v5 Step 1] ì •ëŸ‰ ì ìˆ˜ ê³„ì‚° ({len(candidate_stocks)}ê°œ ì¢…ëª©) - ë¹„ìš© 0ì›")
                    quant_results = {}
                    
                    for code, info in candidate_stocks.items():
                        if code == '0001':
                            continue
                        stock_info = {
                            'code': code,
                            'info': info,
                            'snapshot': snapshot_cache.get(code),
                        }
                        quant_results[code] = process_quant_scoring_task(
                            stock_info, quant_scorer, session, kospi_prices
                        )
                    
                    # Step 2: ì •ëŸ‰ ê¸°ë°˜ 1ì°¨ í•„í„°ë§ (í•˜ìœ„ 20% íƒˆë½) - [v1.1] í•„í„°ë§ ì™„í™”
                    logger.info(f"\n   [v5 Step 2] ì •ëŸ‰ ê¸°ë°˜ 1ì°¨ í•„í„°ë§ (í•˜ìœ„ 20% íƒˆë½)")
                    quant_result_list = list(quant_results.values())
                    filtered_results = quant_scorer.filter_candidates(quant_result_list, cutoff_ratio=0.2)
                    
                    filtered_codes = {r.stock_code for r in filtered_results}
                    logger.info(f"   âœ… ì •ëŸ‰ í•„í„° í†µê³¼: {len(filtered_codes)}ê°œ (í‰ê·  ì ìˆ˜: {sum(r.total_score for r in filtered_results)/len(filtered_results):.1f}ì )")
                    
                    # Step 3: LLM ì •ì„± ë¶„ì„ (í†µê³¼ ì¢…ëª©ë§Œ)
                    logger.info(f"\n   [v5 Step 3] LLM ì •ì„± ë¶„ì„ (í†µê³„ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)")
                    
                    final_approved_list: List[Dict] = []
                    if '0001' in candidate_stocks:
                        final_approved_list.append({'code': '0001', 'name': 'KOSPI', 'is_tradable': False})
                    
                    llm_decision_records: Dict[str, Dict] = {}
                    llm_max_workers = max(1, _parse_int_env(os.getenv("SCOUT_LLM_MAX_WORKERS"), 4))
                    
                    # Phase 1: Hunter (í†µê³„ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
                    phase1_results = []
                    with ThreadPoolExecutor(max_workers=llm_max_workers) as executor:
                        future_to_code = {}
                        for code in filtered_codes:
                            info = candidate_stocks[code]
                            quant_result = quant_results[code]
                            payload = {'code': code, 'info': info}
                            future = executor.submit(
                                process_phase1_hunter_v5_task, 
                                payload, brain, quant_result, snapshot_cache, news_cache
                            )
                            future_to_code[future] = code
                        
                        for future in as_completed(future_to_code):
                            result = future.result()
                            if result:
                                phase1_results.append(result)
                                if not result['passed']:
                                    llm_decision_records[result['code']] = {
                                        'code': result['code'],
                                        'name': result['name'],
                                        'llm_score': result['hunter_score'],
                                        'llm_reason': result['hunter_reason'],
                                        'is_tradable': False,
                                        'approved': False,
                                        'hunter_score': result['hunter_score'],
                                        'llm_metadata': {'llm_grade': 'D', 'source': 'v5_hunter_reject'}
                                    }
                    
                    phase1_passed = [r for r in phase1_results if r['passed']]
                    logger.info(f"   âœ… v5 Hunter í†µê³¼: {len(phase1_passed)}/{len(filtered_codes)}ê°œ")
                    
                    # Phase 2-3: Debate + Judge (ìƒìœ„ ì¢…ëª©ë§Œ)
                    PHASE2_MAX = int(os.getenv("SCOUT_PHASE2_MAX_ENTRIES", "50"))
                    if len(phase1_passed) > PHASE2_MAX:
                        phase1_passed_sorted = sorted(phase1_passed, key=lambda x: x['hunter_score'], reverse=True)
                        phase1_passed = phase1_passed_sorted[:PHASE2_MAX]
                    
                    if phase1_passed:
                        logger.info(f"\n   [v5 Step 4] Debate + Judge (í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê²°í•©)")
                        
                        with ThreadPoolExecutor(max_workers=llm_max_workers) as executor:
                            future_to_code = {}
                            for p1_result in phase1_passed:
                                future = executor.submit(process_phase23_judge_v5_task, p1_result, brain)
                                future_to_code[future] = p1_result['code']
                            
                            for future in as_completed(future_to_code):
                                record = future.result()
                                if record:
                                    llm_decision_records[record['code']] = record
                                    if record.get('approved'):
                                        final_approved_list.append(_record_to_watchlist_entry(record))
                    
                    logger.info(f"   âœ… v5 ìµœì¢… ìŠ¹ì¸: {len([r for r in llm_decision_records.values() if r.get('approved')])}ê°œ")
                    
                    # ì¿¼í„°ì œ ì ìš©
                    MAX_WATCHLIST_SIZE = 15
                    if len(final_approved_list) > MAX_WATCHLIST_SIZE:
                        final_approved_list_sorted = sorted(
                            final_approved_list,
                            key=lambda x: x.get('llm_score', 0),
                            reverse=True
                        )
                        final_approved_list = final_approved_list_sorted[:MAX_WATCHLIST_SIZE]
                    
                    logger.info(f"\n   ğŸ Scout v1.0 ì™„ë£Œ: {len(final_approved_list)}ê°œ ì¢…ëª© ì„ ì •")
                    _v5_completed = True
                    
                except Exception as e:
                    logger.error(f"âŒ Scout v1.0 ì‹¤í–‰ ì˜¤ë¥˜, v4 ëª¨ë“œë¡œ í´ë°±: {e}", exc_info=True)
                    _v5_completed = False
            else:
                _v5_completed = False
            
            # =============================================================
            # [v4.x] ê¸°ì¡´ LLM ê¸°ë°˜ ì„ ì • ë¡œì§ (v5 ë¯¸í™œì„±í™” ë˜ëŠ” ì‹¤íŒ¨ ì‹œ)
            # =============================================================
            if not _v5_completed:
                logger.info("   (Mode) v4.x ê¸°ì¡´ LLM ê¸°ë°˜ ë¡œì§ ì‹¤í–‰")
                
                # [v4.3] ìƒˆë¡œìš´ ìºì‹œ ì‹œìŠ¤í…œ - LLM_EVAL_CACHE í…Œì´ë¸” ê¸°ë°˜ ì§ì ‘ ë¹„êµ (db_conn ì‚¬ìš©)
                llm_cache_snapshot = _load_llm_cache_from_db(session)
                llm_max_workers = max(1, _parse_int_env(os.getenv("SCOUT_LLM_MAX_WORKERS"), 4))
    
                # ì˜¤ëŠ˜ ë‚ ì§œ (KST ê¸°ì¤€)
                kst = timezone(timedelta(hours=9))
                today_str = datetime.now(kst).strftime("%Y-%m-%d")
    
                final_approved_list: List[Dict] = []
                if '0001' in candidate_stocks:
                    final_approved_list.append({'code': '0001', 'name': 'KOSPI', 'is_tradable': False})
                    del candidate_stocks['0001']
    
                llm_decision_records: Dict[str, Dict] = {}
                cache_hits = 0
                pending_codes: List[str] = []
                cache_miss_reasons: Dict[str, str] = {}  # ë””ë²„ê¹…ìš©
    
                for code, info in candidate_stocks.items():
                    cached = llm_cache_snapshot.get(code)
                    
                    # [v4.3] ì§ì ‘ ë¹„êµë¡œ ìºì‹œ ìœ íš¨ì„± ê²€ì¦
                    current_data = {
                        'price_bucket': _get_price_bucket(info.get('price', 0)),
                        'volume_bucket': _get_volume_bucket(info.get('volume', 0)),
                        'news_hash': info.get('news_hash'),
                    }
                    
                    if _is_cache_valid_direct(cached, current_data, today_str):
                        # ìºì‹œ ì ì¤‘ - ì´ì „ LLM ê²°ê³¼ ì¬ì‚¬ìš©
                        llm_decision_records[code] = {
                            'code': code,
                            'name': info['name'],
                            'llm_score': cached.get('judge_score') or cached.get('hunter_score', 0),
                            'llm_reason': cached.get('llm_reason', ''),
                            'is_tradable': cached.get('is_tradable', False),
                            'approved': cached.get('is_approved', False),
                            'llm_metadata': {
                                'llm_grade': cached.get('llm_grade'),
                                'source': 'cache',
                            }
                        }
                        cache_hits += 1
                        if cached.get('is_approved'):
                            final_approved_list.append(_record_to_watchlist_entry(llm_decision_records[code]))
                    else:
                        # ìºì‹œ ë¯¸ìŠ¤ - LLM ì¬í˜¸ì¶œ í•„ìš”
                        pending_codes.append(code)
                        # ë¯¸ìŠ¤ ì›ì¸ ê¸°ë¡ (ë””ë²„ê¹…ìš©)
                        if not cached:
                            cache_miss_reasons[code] = "no_cache"
                        elif cached.get('eval_date') != today_str:
                            cache_miss_reasons[code] = f"date({cached.get('eval_date')}!={today_str})"
                        elif cached.get('price_bucket') != current_data['price_bucket']:
                            cache_miss_reasons[code] = f"price({cached.get('price_bucket')}!={current_data['price_bucket']})"
                        elif (cached.get('news_hash') or '') != (current_data.get('news_hash') or ''):
                            cache_miss_reasons[code] = "news_changed"
    
                if cache_hits:
                    logger.info(f"   (LLM) âœ… ìºì‹œ ì ì¤‘ {cache_hits}ê±´ (ì˜¤ëŠ˜ ë‚ ì§œ + ë™ì¼ ê°€ê²©/ë‰´ìŠ¤)")
                
                if pending_codes:
                    # ìºì‹œ ë¯¸ìŠ¤ ì›ì¸ ë¶„ì„
                    reason_counts = {}
                    for reason in cache_miss_reasons.values():
                        reason_type = reason.split("(")[0]
                        reason_counts[reason_type] = reason_counts.get(reason_type, 0) + 1
                    logger.info(f"   (LLM) âš ï¸ ìºì‹œ ë¯¸ìŠ¤ {len(pending_codes)}ê±´ - ì›ì¸: {reason_counts}")
    
                need_llm_calls = len(pending_codes) > 0
    
                llm_invocation_count = 0
                if need_llm_calls:
                    if brain is None:
                        logger.error("   (LLM) JennieBrain ì´ˆê¸°í™” ì‹¤íŒ¨ë¡œ ì‹ ê·œ í˜¸ì¶œì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        # [v3.8] 2-Pass ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
                        # Pass 1: Phase 1 Hunter (Gemini-Flash) - ë³‘ë ¬ë¡œ ë¹ ë¥´ê²Œ í•„í„°ë§
                        logger.info(f"   (LLM) [Pass 1] Phase 1 Hunter ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘ ({len(pending_codes)}ê°œ ì¢…ëª©)")
                        update_pipeline_status(
                            phase=1, phase_name="Hunter Scout", status="running",
                            total_candidates=len(candidate_stocks)
                        )
                        phase1_start = time.time()
                        
                        phase1_results = []
                        # [v4.1] Claude Rate Limit ëŒ€ì‘: ì›Œì»¤ ìˆ˜ ì œí•œ (ê¸°ì¡´ *2 ì œê±°)
                        phase1_worker_count = min(llm_max_workers, max(1, len(pending_codes)))
                        logger.info(f"   (LLM) Phase 1 ì›Œì»¤ ìˆ˜: {phase1_worker_count}ê°œ (Rate Limit ëŒ€ì‘)")
                        
                        with ThreadPoolExecutor(max_workers=phase1_worker_count) as executor:
                            future_to_code = {}
                            for code in pending_codes:
                                payload = {
                                    'code': code,
                                    'info': candidate_stocks[code],
                                }
                                # [v4.2] ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒí•˜ë„ë¡ ë³€ê²½ (API í˜¸ì¶œ X)
                                future = executor.submit(process_phase1_hunter_task, payload, brain, snapshot_cache, news_cache)
                                future_to_code[future] = code
                            
                            for future in as_completed(future_to_code):
                                result = future.result()
                                if result:
                                    phase1_results.append(result)
                                    # Phase 1 íƒˆë½ ì¢…ëª©ë„ ê¸°ë¡ (ìºì‹œìš©)
                                    if not result['passed']:
                                        llm_decision_records[result['code']] = {
                                            'code': result['code'],
                                            'name': result['name'],
                                            'is_tradable': False,
                                            'llm_score': result['hunter_score'],
                                            'llm_reason': result['hunter_reason'] or 'Phase 1 í•„í„°ë§ íƒˆë½',
                                            'approved': False,
                                            'hunter_score': result['hunter_score'],  # [v4.3] ìºì‹œ ì €ì¥ìš©
                                            'llm_metadata': {
                                                'llm_grade': 'D',
                                                'llm_updated_at': _utcnow().isoformat(),
                                                'source': 'llm_hunter_reject',
                                            }
                                        }
                        
                        phase1_passed_all = [r for r in phase1_results if r['passed']]
                        phase1_time = time.time() - phase1_start
                        logger.info(f"   (LLM) [Pass 1] Phase 1 ì™„ë£Œ: {len(phase1_passed_all)}/{len(pending_codes)}ê°œ í†µê³¼ ({phase1_time:.1f}ì´ˆ)")
                        
                        # [v4.1] Phase 2 ì§„ì… ì œí•œ: ìƒìœ„ 50ê°œë§Œ (ì†ë„ ìµœì í™”)
                        PHASE2_MAX_ENTRIES = int(os.getenv("SCOUT_PHASE2_MAX_ENTRIES", "50"))
                        if len(phase1_passed_all) > PHASE2_MAX_ENTRIES:
                            phase1_passed_sorted = sorted(phase1_passed_all, key=lambda x: x['hunter_score'], reverse=True)
                            phase1_passed = phase1_passed_sorted[:PHASE2_MAX_ENTRIES]
                            logger.info(f"   (LLM) [ì†ë„ ìµœì í™”] Phase 2 ì§„ì… ì œí•œ: ìƒìœ„ {PHASE2_MAX_ENTRIES}ê°œë§Œ ì„ íƒ (ì „ì²´ {len(phase1_passed_all)}ê°œ ì¤‘)")
                        else:
                            phase1_passed = phase1_passed_all
                        
                        # [v1.0] Redis ìƒíƒœ ì—…ë°ì´íŠ¸ - Phase 1 ì™„ë£Œ
                        update_pipeline_status(
                            phase=2, phase_name="Bull vs Bear Debate", status="running",
                            total_candidates=len(candidate_stocks),
                            passed_phase1=len(phase1_passed_all)  # ì „ì²´ í†µê³¼ ìˆ˜ í‘œì‹œ
                        )
                        
                        # Pass 2: Phase 2-3 Debate+Judge (GPT-5-mini) - ìƒìœ„ ì¢…ëª©ë§Œ
                        if phase1_passed:
                            logger.info(f"   (LLM) [Pass 2] Phase 2-3 Debate-Judge ì‹¤í–‰ ({len(phase1_passed)}ê°œ ì¢…ëª©)")
                            phase23_start = time.time()
                            
                            phase23_worker_count = min(llm_max_workers, max(1, len(phase1_passed)))
                            
                            with ThreadPoolExecutor(max_workers=phase23_worker_count) as executor:
                                future_to_code = {}
                                for phase1_result in phase1_passed:
                                    future = executor.submit(process_phase23_debate_judge_task, phase1_result, brain)
                                    future_to_code[future] = phase1_result['code']
                                
                                for future in as_completed(future_to_code):
                                    record = future.result()
                                    if not record:
                                        continue
                                    llm_invocation_count += 1
                                    llm_decision_records[record['code']] = record
                                    if record.get('approved'):
                                        final_approved_list.append(_record_to_watchlist_entry(record))
                            
                            phase23_time = time.time() - phase23_start
                            logger.info(f"   (LLM) [Pass 2] Phase 2-3 ì™„ë£Œ ({phase23_time:.1f}ì´ˆ)")
                            
                            # [v1.0] Redis ìƒíƒœ ì—…ë°ì´íŠ¸ - Phase 2-3 ì™„ë£Œ
                            update_pipeline_status(
                                phase=3, phase_name="Final Judge", status="running",
                                total_candidates=len(candidate_stocks),
                                passed_phase1=len(phase1_passed),
                                passed_phase2=len(phase1_passed),  # Debateì€ ì „ì› ì°¸ì—¬
                                final_selected=len(final_approved_list)
                            )
                        else:
                            logger.info("   (LLM) [Pass 2] Phase 1 í†µê³¼ ì¢…ëª© ì—†ìŒ, Phase 2-3 ê±´ë„ˆëœ€")
                else:
                    logger.info("   (LLM) ëª¨ë“  í›„ë³´ê°€ ìºì‹œë¡œ ì¶©ì¡±ë˜ì–´ ì‹ ê·œ í˜¸ì¶œì´ ì—†ìŠµë‹ˆë‹¤.")
    
                logger.info("   (LLM) ì‹ ê·œ í˜¸ì¶œ ìˆ˜: %d", llm_invocation_count)
    
                # [v4.3] ìƒˆë¡œìš´ ìºì‹œ í…Œì´ë¸”ì— ê²°ê³¼ ì €ì¥
                if llm_invocation_count > 0:
                    new_cache_entries = {}
                    for code, record in llm_decision_records.items():
                        info = candidate_stocks.get(code, {})
                        new_cache_entries[code] = {
                            'stock_name': record.get('name', ''),
                            'price_bucket': _get_price_bucket(info.get('price', 0)),
                            'volume_bucket': _get_volume_bucket(info.get('volume', 0)),
                            'news_hash': info.get('news_hash'),
                            'eval_date': today_str,
                            'hunter_score': record.get('hunter_score', record.get('llm_score', 0)),
                            'judge_score': record.get('llm_score', 0),
                            'llm_grade': record.get('llm_metadata', {}).get('llm_grade'),
                            'llm_reason': record.get('llm_reason', '')[:60000] if record.get('llm_reason') else None,
                            'news_used': news_cache.get(code, '')[:60000] if news_cache.get(code) else None,
                            'is_approved': record.get('approved', False),
                            'is_tradable': record.get('is_tradable', False),
                        }
                    _save_llm_cache_batch(session, new_cache_entries)
                    _save_last_llm_run_at(session, _utcnow())
    
                # [v1.0] Phase 3: ì¿¼í„°ì œ ì ìš© (Top 15ê°œë§Œ ì €ì¥) - ì œë‹ˆ í”¼ë“œë°± ë°˜ì˜
                MAX_WATCHLIST_SIZE = 15
                
                # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ìƒìœ„ Nê°œë§Œ ì„ íƒ
                if len(final_approved_list) > MAX_WATCHLIST_SIZE:
                    final_approved_list_sorted = sorted(
                        final_approved_list, 
                        key=lambda x: x.get('llm_score', 0), 
                        reverse=True
                    )
                    final_approved_list = final_approved_list_sorted[:MAX_WATCHLIST_SIZE]
                    logger.info(f"   (ì¿¼í„°ì œ) ìƒìœ„ {MAX_WATCHLIST_SIZE}ê°œë§Œ ì„ ì • (ì´ {len(final_approved_list_sorted)}ê°œ ì¤‘)")
            
            # =============================================================
            # [ê³µí†µ] Phase 3: ìµœì¢… Watchlist ì €ì¥
            # =============================================================
            logger.info(f"--- [Phase 3] ìµœì¢… Watchlist {len(final_approved_list)}ê°œ ì €ì¥ ---")
            database.save_to_watchlist(session, final_approved_list)
            
            with ThreadPoolExecutor(max_workers=10) as executor:
                if hasattr(kis_api, 'API_CALL_DELAY'):
                    future_to_data = {
                        executor.submit(fetch_kis_data_task, s, kis_api): (time.sleep(kis_api.API_CALL_DELAY), s)[1]
                        for s in final_approved_list 
                    }
                else:
                    future_to_data = {
                        executor.submit(fetch_kis_data_task, s, kis_api): s
                        for s in final_approved_list 
                    }
                
                all_daily = []
                all_fund = []
                for future in as_completed(future_to_data):
                    d, f = future.result()
                    if d: all_daily.extend(d)
                    if f: all_fund.append(f)
            
            if all_daily: database.save_all_daily_prices(session, all_daily)
            if all_fund: database.update_all_stock_fundamentals(session, all_fund)
            
            # Phase 3-A: ì¬ë¬´ ë°ì´í„° (ë„¤ì´ë²„ í¬ë¡¤ë§)
            tradable_codes = [s['code'] for s in final_approved_list if s.get('is_tradable', True)]
            if tradable_codes:
                batch_update_watchlist_financial_data(session, tradable_codes)
            
            # [v1.0] Redis ìµœì¢… ìƒíƒœ ì—…ë°ì´íŠ¸ - ì™„ë£Œ
            update_pipeline_status(
                phase=3, phase_name="Final Judge", status="completed",
                progress=100,
                total_candidates=len(candidate_stocks) if 'candidate_stocks' in locals() else 0,
                passed_phase1=len(phase1_passed) if 'phase1_passed' in locals() else 0,
                passed_phase2=len(phase1_passed) if 'phase1_passed' in locals() else 0,
                final_selected=len(final_approved_list)
            )
            
            # [v1.0] Redis ê²°ê³¼ ì €ì¥ (Dashboardì—ì„œ ì¡°íšŒìš©)
            pipeline_results = [
                {
                    "stock_code": s.get('code'),
                    "stock_name": s.get('name'),
                    "grade": s.get('llm_metadata', {}).get('llm_grade', 'C'),
                    "final_score": s.get('llm_score', 0),
                    "selected": s.get('approved', False),
                    "judge_reason": s.get('llm_reason', ''),
                }
                for s in final_approved_list
            ]
            save_pipeline_results(pipeline_results)
            logger.info(f"   (Redis) Dashboardìš© ê²°ê³¼ ì €ì¥ ì™„ë£Œ ({len(pipeline_results)}ê°œ)")

    except Exception as e:
        logger.critical(f"âŒ 'Scout Job' ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        # [v1.0] ì˜¤ë¥˜ ì‹œ Redis ìƒíƒœ ì—…ë°ì´íŠ¸
        update_pipeline_status(phase=0, phase_name="Error", status="error")
            
    logger.info(f"--- ğŸ¤– 'Scout Job' ì¢…ë£Œ (ì†Œìš”: {time.time() - start_time:.2f}ì´ˆ) ---")

if __name__ == "__main__":
    main()
